\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Idioma e tipografia
\usepackage[brazilian]{babel}
\usepackage{csquotes}
\usepackage{lmodern}
\usepackage{microtype}

% Layout e recursos básicos
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bookmark}

\emergencystretch=2em
\cleardoublepage
\pagenumbering{arabic}

% Matemática e teoremas (essencial só se você usa)
\usepackage{amsmath,amssymb,amsthm}

% Tabelas em paisagem e colunas flexíveis
% \usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{array} % para \newcolumntype
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
\usepackage{rotating} % para sidewaystable/sideways


% Bibliografia ABNT numerada
\usepackage[
  backend=biber,
  style=abnt,
  sorting=none,
  giveninits=true,
  uniquename=false,
  doi=false,
  isbn=false,
  url=false,
  language=brazil,
  scbib,
  ittitles,
  justify
]{biblatex}
\addbibresource{refs.bib} % ← caminho corrigido



% ======= PADRONIZAÇÃO PARA A TABELA MDRE =======

% Coluna flexível "Y" (se ainda não tiver)
% \usepackage{tabularx,booktabs,ragged2e,array}
% \newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}

% 1) Vocabulário controlado (sempre em SMALL CAPS):
% force medium series inside \textsc to avoid requesting a bold small-caps font (bx/sc)
\newcommand{\Static}{{\mdseries\textsc{Estático}}}
\newcommand{\Dynamic}{{\mdseries\textsc{Dinâmico}}}
\newcommand{\Hybrid}{{\mdseries\textsc{Híbrido}}}
\newcommand{\Comp}{{\mdseries\textsc{Compreensão}}}
\newcommand{\Redoc}{{\mdseries\textsc{Redocumentação}}}
\newcommand{\Mig}{{\mdseries\textsc{Migração}}}
\newcommand{\Quali}{{\mdseries\textsc{Qualidade}}}

% 2) Macros para setas e encadeamentos:
\newcommand{\ctoa}{\(\text{Código} \rightarrow \text{AST}\)}
\newcommand{\atoxi}{\(\text{AST} \rightarrow \text{IM}\)}   % IM = modelo intermediário
\newcommand{\imtoxml}{\(\text{IM} \rightarrow \text{XML}\)}
\newcommand{\imtomdl}{\(\text{IM} \rightarrow \text{UML}\)}
\newcommand{\tmtomdl}{\(\text{T2M/M2M} \rightarrow \text{UML}\)}
\newcommand{\xtoSeq}{\(\rightarrow \text{UML Sequência}\)}
\newcommand{\xtoClass}{\(\rightarrow \text{UML Classe}\)}
\newcommand{\xtoAct}{\(\rightarrow \text{UML Atividade}\)}

% 3) Abreviações de ferramentas (consistentes):
\newcommand{\EMF}{Eclipse/EMF}
\newcommand{\UMLtwo}{UML2}
\newcommand{\PlantUML}{PlantUML}
\newcommand{\JavaParser}{JavaParser}

% 4) Formato da célula “Aspecto”: Técnica ; Objetivo(s)
%    Ex.: \Static; \Comp/\Redoc (estrutura + comportamento)

% 5) Formato da célula “Técnica/Transformação”:
%    Use sempre cadeia com “→”, negrite elementos-chaves e padronize nomes.
%    Ex.: Código → AST → \textbf{IM(XML)} → T2M/M2M → \textbf{UML2}
%
% 6) Formato da célula “Validação”:
%    [tipo de evidência; dataset/projetos; métrica(s) ou avaliação; nota curta]
%    Ex.: OSS (9 projetos, 2640 classes); AUC=0.73; custo de rótulo 10%

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{0cm}
        
            \includegraphics[width=0.5\textwidth]{Images/Logo_FGV.png} 
            
        \vspace{1.5cm}
        \large
        
        Ciência de Dados e I.A.\\
        Escola de Matemática Aplicada\\
        Fundação Getúlio Vargas\\

        \vspace{1cm}  
    
        \Large
        Engenharia de Requisitos
            
        \vspace{2cm}
        
        \vspace{0.25cm}

        \Huge \textbf{Proposta de TCC} \\ 
        \vspace{0.5cm}
        \huge \textbf{LLM para Engenharia de Requisitos}
        \vspace{3.6cm}
        
        \large
                Aluno: Isabela Yabe\\
                Orientador: Rafael de Pinho André\\
                Escola de Matemática Aplicada, FGV/EMAp \\
                Rio de Janeiro - RJ.
        \vfill
            
        \vspace{0.8cm}  
        
        Rio de Janeiro, 2025
            
    \end{center}
\end{titlepage}
\newpage
\pagenumbering{roman}
\tableofcontents

\newpage
\pagenumbering{arabic}

\section{Resumo}
 Este trabalho investiga se é possível recuperar artefatos de requisitos, em particular diagramas de casos de uso, diretamente a partir de sistemas implementados em Python, combinando análise estática de código e técnicas recentes de representação semântica com \textit{Large Language Models} (LLMs).

A proposta parte da \textit{Abstract Syntax Tree} (AST) do código-fonte, tratada como modelo intermediário em um fluxo de \textit{Model-Driven Reverse Engineering} (MDRE). Sobre essa estrutura, são extraídas \textit{features} estruturais (tipo de nó, escopo, relações de chamada) e textuais (nomes, docstrings, comentários), que alimentam um encoder de nós e uma GNN responsável por produzir embeddings semântico-estruturais do arquivo. A partir desses embeddings, métodos públicos são identificados como candidatos a casos de uso e agrupados por similaridade, enquanto o grafo de chamadas fornece relações de dependência entre casos.

O resultado esperado é um processo de redocumentação capaz de gerar diagramas de casos de uso em \textit{PlantUML} a partir de código Python, preservando a semântica observada e oferecendo uma visão de alto nível do sistema. A principal contribuição é aproximar engenharia de requisitos e engenharia reversa ao mostrar como LLMs e modelos de código orientados por AST podem apoiar a recuperação de requisitos em cenários em que a documentação está ausente ou desatualizada.

\section{Introdução}

A engenharia de software estuda e avalia métodos capazes de aproximar o código-fonte da linguagem natural. Essa busca se manifesta em duas vertentes complementares: a interação com o usuário final e a comunicação entre os próprios desenvolvedores. 

Este estudo fundamenta-se em autores que defendem o desenvolvimento estruturado e orientado ao usuário, projetado a partir da visão e das necessidades de quem utiliza o sistema, e não apenas da estrutura interna ou das preferências de quem o desenvolve. Essa perspectiva deu origem a princípios de design centrados na função e no comportamento observável do sistema, enfatizando que a organização do código deve refletir a experiência do usuário e os fluxos de interação previstos. 

\textcite{yourdon1979structured} descrevem o processo tradicional de desenvolvimento de software como uma cadeia de tradução sucessiva: o diálogo entre o proprietário do produto, o usuário e o analista é continuamente reinterpretado pelo engenheiro de requisitos, pelo designer e pelo programador, conforme ilustrado na Figura~\ref{fig:cadeia_traducao_constantine1979}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{Images/diagrams/fluxo_info.png}
  \caption{cadeia de tradução de requisitos segundo Constantine \(1979\).}
  \label{fig:cadeia_traducao_constantine1979}
\end{figure}

Cada etapa dessa cadeia implica a perda ou distorção de parte do significado original do usuário, o que pode resultar em comportamentos apenas próximos ao desejado. Diante disso, os autores propõem o projeto estruturado, cujo ponto inicial é a clareza e a visibilidade das decisões e atividades envolvidas, promovendo uma compreensão compartilhada e garantindo que o design reflita as intenções originais do sistema.

\subsection{Problematização}

Com o mesmo intuito de tornar o comportamento do sistema visível e compreensível, surge a modelagem de casos de uso como um instrumento de unificação entre requisitos, design e usabilidade. Segundo \textcite{booch1999unified}, nenhum sistema existe isoladamente: todo sistema relevante interage com atores, humanos ou automáticos, que esperam comportamentos previsíveis. O diagrama de casos de uso permite que analistas e desenvolvedores discutam o comportamento do sistema sem se prender aos detalhes da implementação, oferecendo uma linguagem comum e verificável para representar comportamentos.

Autores posteriores ampliaram essa discussão para o nível do código, enfatizando a necessidade de que o código não seja apenas executável, mas também compreensível. Como sintetiza \textcite{fowler2018refactoring}, “qualquer tolo escreve um código que um computador possa entender; bons programadores escrevem código que seres humanos possam entender”.

Entretanto, a legibilidade do código, por si só, não substitui a documentação de requisitos. Enquanto o código explica como o sistema se comporta, a documentação torna explícito por que ele deve se comportar assim. Segundo \textcite{sommerville1997requirements}, a documentação de requisitos atua como um contrato conceitual entre usuários, analistas e desenvolvedores, garantindo o alinhamento entre o comportamento implementado e as expectativas de negócio. Quando essa documentação falta ou envelhece, a legibilidade do código torna-se o principal ponto de apoio para reconstruir as intenções originais, o que representa um desafio na manutenção e evolução de sistemas legados.

\subsection{Questão e hipótese}

Se o código é um texto escrito para ser lido por humanos, então suas palavras, nomes e estruturas carregam pistas úteis sobre o que o sistema faz e para quem. Partindo dessa premissa, pergunta-se: é possível reconstruir casos de uso a partir do código-fonte, combinando análise estrutural e interpretação semântica automatizada?

A hipótese deste trabalho é que técnicas de representação semântica, como embeddings e \textit{Large Language Models} (LLMs), quando aplicadas sobre estruturas abstratas do código, como a \textit{Abstract Syntax Tree} (AST), podem viabilizar a reconstrução de artefatos de alto nível, como diagramas de casos de uso, mesmo na ausência de documentação formal.

\subsection{Objetivos}

O objetivo geral deste trabalho é propor um processo de redocumentação automatizada capaz de gerar diagramas de casos de uso a partir do código-fonte, preservando a semântica do sistema original.
Para isso, o método combina:

\textcite{Bruneliere2010MoDisco} o MoDisco, um framework genérico para engenharia reversa orientada por modelos (Model-Driven Reverse Engineering — MDRE). Ele sugere resumirmos os sistemas em modelos, uma estrutura mais homogênea. A principal ideia é recuperar modelos existentes no sistema. O processo é dividido em duas fases, descoberta do modelo e compreensão do modelo. Na fase de descoberta, um componente chamado discoverer extrai informações do código-fonte, dados brutos, documentações e artefatos disponíveis. Passando estas informações para uma representação da estrutura do sistema. Já na fase de compreensão, o conteúdo desse modelo é analisado e transformado em representações de alto nível, diagramas, métricas ou relatórios, que podem servir à redocumentação, à modernização de sistemas ou à análise de qualidade.

A partir dessa arquitetura, adotaremos a mesma lógica de abstração proposta por \textcite{tonella2007reverse}, utilizando uma representação sintática reduzida do código-fonte que preserva apenas os elementos essenciais ao fluxo de objetos, criações, atribuições e chamadas, e ignora instruções de controle. Essa simplificação torna possível construir a Abstract Syntax Tree (AST) como modelo intermediário, permitindo representar a estrutura e os diagramas de casos de uso.

Esse tipo de investigação é definido por \textcite{chikofsky1990reverse} como \textit{Redocumentation} em \textit{Reverse engineering}, ou seja, engenharia reversa com foco em redocumentação, no sentido de criar representações de abstração do sistema existente, destinadas à leitura humana, sem alterar o comportamento do software. 

Além da linguagem abstrata, este trabalho incorpora informações semânticas extraídas diretamente das \textit{docstrings}, comentários e nomenclaturas do código. Esses elementos textuais são tratados como extensões dos objetos, pois também comunicam intenções, objetivos e relações entre entidades. Com o apoio de \textit{Large Language Models} (LLMs), essas evidências são analisadas de forma contextual, permitindo inferir papéis, objetivos e interações que não estão explicitamente representados nas chamadas ou estruturas do código.

Dessa forma, o processo de redocumentação combina a análise estrutural, que descreve como os objetos estão correlacionados, e a análise semântica, que interpreta o vocabulário interno do sistema revelando as intenções dos desenvolvedores. 

\subsection{Relevância}

Este trabalho contribui para auxiliar desenvolvedores durante a codificação e também na compreensão de sistemas sem documentação. Ao gerar visões de alto nível do sistema, especificamente casos de uso, a proposta facilita a compreensão e as interações entre componentes. 

Segundo \textcite{larman2002applying}, os casos de uso não apenas documentam funcionalidades, mas representam um instrumento de convergência entre analistas, projetistas e programadorea. Em contextos dinâmicos, casos de uso bem definidos apoiam a priorização de requisitos, a validação de comportamentos e a manutenção de uma visão compartilhada do sistema, mesmo diante de mudanças constantes.

Embora a maioria dos estudos sobre Model-Driven Reverse Engineering (MDRE) e redocumentação concentre-se em linguagens como Java, este trabalho propõe uma abordagem direcionada à linguagem Python, que, segundo o TIOBE Index (2025), mantém-se como a linguagem mais popular globalmente. 

Por fim, além de oferecer uma nova aplicação prática de Large Language Models na engenharia de software, o estudo propõe uma ponte entre engenharia de requisitos e engenharia reversa, reforçando a ideia de que compreender um sistema começa por compreender seu código, não apenas como sequência de instruções, mas como expressão das intenções humanas que lhe deram origem.

\section{Revisão literária}

A revisão tem o objetivo de compreender o estado da arte das abordagens de engenharia reversa que partem de código-fonte e produzem artefatos de alto nível, como diagramas UML. Para garantir uma análise sistemática e comparável entre diferentes propostas, foram definidas perguntas de pesquisa (\textit{Research Questions — RQs}) que orientam a coleta e síntese dos dados extraídos dos estudos selecionados.

\begin{itemize}
  \item \textbf{RQ1.} Em quais linguagens e domínios as abordagens que partem de código-fonte foram aplicadas?
  \item \textbf{RQ2.} Quais modelos/artefatos de alto nível são gerados?
  \item \textbf{RQ3.} Qual aspecto é privilegiado (estático, dinâmico, híbrido) e com qual objetivo (compreensão, redocumentação, migração, qualidade)?
  \item \textbf{RQ4.} Quais técnicas e transformações viabilizam a passagem do código para o modelo de alto nível?
  \item \textbf{RQ5.} Quais ferramentas/frameworks são utilizados?
  \item \textbf{RQ6.} Como as abordagens são validadas e com que qualidade prática?
\end{itemize}

A coleta dos estudos seguiu uma estratégia sistemática de busca em bases reconhecidas, IEEE Xplore e ACM Digital Library no período de 2015 a 2025.

A query se estrutura na combinação de três blocos temáticos:

\begin{itemize}
  \item ("Abstract": "MDRE" OR "reverse engineering" OR "model driven reverse engineering" OR "design recovery")
  \item ("Abstract": "UML" OR "UML class diagram" OR "UML activity diagram" OR "UML sequence diagram" OR "UML models" OR "Diagram")
  \item: ("Abstract": "static analysis" OR "source code analysis" OR "abstract syntax tree" OR "AST" OR "text-to-model" OR "T2M" OR "parser" OR "source code" OR "parsing")
\end{itemize}

Foram incluídos apenas os estudos que propõem uma abordagem de engenharia reversa aplicada à geração de modelos UML (classes, atividades ou sequência) diretamente a partir do código-fonte.
  
Foram excluídos os trabalhos que se enquadravam em uma ou mais das seguintes categorias:
\begin{itemize}
  \item Foco em forward engineering ou geração de código.
  \item Estudos centrados em rastreabilidade ou anti-padrões.
  \item Trabalhos puramente empíricos ou teóricos sem proposta de transformação automatizada.
  \item Abordagens puramente dinâmicas.
\end{itemize}

Com base nos critérios de inclusão e exclusão, foram selecionados os seguintes estudos para análise detalhada:

\begin{itemize}
  \item A Model Driven Reverse Engineering Framework for Generating High Level UML Models From Java Source Code (2019).
  \item Condensing Class Diagrams With Minimal Manual Labeling Cost (2016). (parte do diagrama e aperfeiçoa)
  \item Enhancing Model-Driven Reverse Engineering Using Machine Learning (2024).
  \item Reverse Engineering of Source Code to Sequence Diagram Using Abstract Syntax Tree (2016).
  \item Towards a New Hybrid Approach of the Reverse Engineering of UML Sequence Diagram (2016).
  \item WIP: Generating Sequence Diagrams for Modern Fortran (2017).
\end{itemize}

\begin{sidewaystable}[p]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabularx}{\textheight}{Y Y Y Y Y Y Y}
\toprule
\textbf{Autores / Referência} &
\textbf{Linguagem / Domínio} &
\textbf{Modelo Gerado} &
\textbf{Aspecto} &
\textbf{Técnica / Tipo de Transformação} &
\textbf{Ferramenta / Framework} &
\textbf{Validação / Estudo de Caso} \\
\midrule

\textcite{zhang2016j2x} &
Java; pequenos sistemas OO (eLib, Minesweeper, Blog, PayrollSys, myAlgsLib) &
UML Classe; UML Sequência &
\Static; \Comp/\Redoc &
Código → AST → J2X; mapeamentos (gen./impl./assoc./dep.); sentenças simplif. → OFG; CFG + OFG → UML Sequência &
J2UML; JavaCC; DOM4J; J2X (DTD/XML) &
5 sistemas pequenos; acurácia: classes 96,4–100\%; relações 65,0–90,4\% \\

\textcite{yang2016condensing} &
Java; sistemas OO &
UML Classe (condensado) &
\Static; \Comp/\Redoc (estrutura) &
Extração de métricas (SDMetrics) → normalização (z-score) → k-means → under-sampling → ensemble (Random Forest) → diagrama condensado &
MagicDraw; SDMetrics; Random Forest; Windows 7 &
OSS (9 projetos, 2640 classes); AUC = 0.73; custo de rótulo = 10\%; testes de Wilcoxon e Cliff’s $\delta$ \\

\textcite{Fauzi2016AST} &
Java; sistemas OO &
UML Sequência (comportamental) &
\Static; \Comp/\Redoc &
Código → AST (\JavaParser) → DFS pós-ordem → \PlantUML{} (Sequência) &
REVUML; \JavaParser; \PlantUML &
126 casos de teste (8 categorias); geração correta e consistente de diagramas \\

\textcite{baidada2016hybrid} &
Java / genérico; aplicações OO &
UML Sequência (HLSD) &
\Hybrid{}; \Comp/\Redoc (comportamento) &
CFG → geração de entradas; execuções + traços (filtragem) → CPN (IR) → UML Sequência &
UML2; instrumentação / JVM / debugger; CPN (IR) & 
Sem validação empírica reportada \\

\textcite{leatongkam2017generating} &
Fortran OO; computação científica e engenharia &
UML Classe; UML Sequência; XMI intermediário &
\Static; \Comp/\Redoc &
Regras código → UML (OMG); parsing estático; árvore sintática; geração XMI → importação ArgoUML &
ForUML (extensão); ArgoUML; OMG UML/XMI &
\textit{Work in progress} \\

\textcite{Sabir2019MDRE} &
Java; sistemas legados OO &
UML Class Diagram; UML Activity Diagram &
\Static; \Comp/\Redoc &
T2M/M2M: Parser → AST → IM(XML) → mapeamentos IM → UML (classe/atividade) &
Eclipse + UML2/EMF; JavaParser (IMD); Papyrus / StarUML / Rational Rose (validação) &
5 estudos de caso; comparação especialista (ATM e Amadeus) \\

\textcite{siala2024enhancing} &
Java; Python; sistemas legados &
UML Classe; OCL &
\Static; \Comp/\Redoc/\Mig &
Código → tokenização / simplificação → representação textual UML/OCL (LLM) → model repair → UML/OCL &
Graphviz; \PlantUML; Modelio; AgileUML; LLM &
2 estudos de caso; avaliação de correção semântica, completude e compreensibilidade \\
\bottomrule

\end{tabularx}

\caption{Síntese comparativa dos estudos selecionados.}
\label{tab:sintese_comparativa}
\end{sidewaystable}

\subsection{Análise sistemática da literatura}

A partir da síntese da Tabela~\ref{tab:sintese_comparativa}, organizamos os achados por eixo (RQ1–RQ6).

\paragraph{RQ1 — Linguagens e domínios.}
Predomina o ecossistema \textbf{Java} em sistemas orientados a objetos, tanto em estudos estruturais quanto comportamentais (\textcite{zhang2016j2x, yang2016condensing, Fauzi2016AST, Sabir2019MDRE}). Há ampliação pontual para \textbf{Fortran OO} em contexto de computação científica (\textcite{leatongkam2017generating}) e menção tanto a \textbf{Java} quanto a \textbf{Python} em proposta recente com LLMs (\textcite{siala2024enhancing}). Em síntese, o corpus avaliado é fortemente dominado por Java, Python surge como alvo ainda subexplorado.

\paragraph{RQ2 — Modelos/artefatos gerados.}
A produção concentra-se em \textbf{UML Classe} e \textbf{UML Sequência}. Em \textcite{zhang2016j2x}, ambos são gerados a partir de um pipeline \textit{código} $\rightarrow$ \textit{J2X} $\rightarrow$ \textit{OFG/CFG} $\rightarrow$ \textit{UML} (Classe+Sequência) . \textcite{leatongkam2017generating} propõe estender o \emph{ForUML} para também extrair \textbf{Sequência} a partir de Fortran OO, exportando um \textbf{XMI} intermediário para visualização (e.g., ArgoUML) . \textcite{Fauzi2016AST} derivam \textbf{Sequência} diretamente da \textbf{AST}, com saída em \emph{PlantUML} (Seq), abordagem estática focada em interações. \textcite{Sabir2019MDRE} incluem \textbf{Activity} além de \textbf{Class}, gerando “modelos UML de alto nível” (classe + atividade) a partir de um modelo intermediário (UML2) . \textcite{yang2016condensing} não “geram” um novo tipo de diagrama, mas \emph{condensam} \textbf{diagramas de classe} via métricas + \emph{ensemble learning}, reduzindo a complexidade visual ao destacar “classes importantes” (AUC, testes de Wilcoxon/Cliff’s $\delta$) . Em contraste, \textbf{Casos de Uso} aparecem sobretudo como enquadramento conceitual para Sequência, mas não como artefato recuperado dos códigos analisados, sinalizando uma lacuna na redocumentação de requisitos a partir de código.

\paragraph{RQ3 — Qual aspecto é privilegiado (estático, dinâmico, híbrido) e com qual objetivo?}
No conjunto analisado, prevalece de forma nítida o aspecto \textbf{\Static}, quase sempre orientado à \textbf{\Comp/\Redoc}. Os trabalhos clássicos da vertente estrutural e comportamental, como \textcite{zhang2016j2x} e \textcite{Fauzi2016AST}, operam integralmente sobre o código (sem execução), partindo de \emph{parsing}/AST e passando por representações intermediárias (p.\,ex., J2X) ou travessias específicas (DFS pós-ordem) para derivar, respectivamente, diagramas de Classe e Sequência. Em \textcite{leatongkam2017generating}, a mesma orientação estática se mantém ao estender o ForUML para Fortran OO via regras de mapeamento e exportação XMI; e em \textcite{Sabir2019MDRE}, trata-se de um processo estático T2M com fluxo código$\rightarrow$AST$\rightarrow$IM$\rightarrow$UML, com objetivo voltado à compreensão e redocumentação. Ainda sob a ótica estática, \textcite{yang2016condensing} não cria um novo artefato, mas trata o \emph{pós-processamento} do diagrama de classes via métricas e \emph{machine learning}, reduzindo a complexidade visual sem recorrer a dados de execução. Em contraste com esse predomínio, \textcite{baidada2016hybrid} introduzem um caminho \textbf{\Hybrid} (estático + dinâmico) para Sequência: gera-se um conjunto de entradas a partir do CFG, coletam-se traços por instrumentação/VM, sintetiza-se uma IR (\textit{Intermeadiate Representation}) comportamental em \emph{Colored Petri Nets} e, então, mapeia-se para UML. Por fim, \textcite{siala2024enhancing} preserva o caráter \textbf{\Static} ao integrar LLMs como camada semântica (texto intermediário UML/OCL seguido de \emph{model repair}), ampliando o objetivo para \textbf{migração} em sistemas legados e apontando uma inflexão do “estrutural puro” para um \emph{estrutural + semântico}. 

\paragraph{RQ4 — Técnicas e transformações.}
As abordagens convergem em um esqueleto MDRE que encadeia \emph{Text-to-Model} e \emph{Model-to-Model}: análise sintática do código (\emph{parsing}) para \textbf{AST} ou \textbf{IR} textual, seguida de mapeamentos para o metamodelo UML. Ainda assim, diferem nos \emph{intermediários}, nos operadores de fluxo usados para recuperar comportamento e no quanto incorporam \emph{semântica} além da sintaxe. Em \textcite{zhang2016j2x}, o núcleo é a \textbf{J2X} (DTD/XML), uma IR que padroniza elementos de linguagem; o diagrama de classes surge de metadados extraídos dessa IR, enquanto o diagrama de sequência resulta da \textbf{integração OFG+CFG} (rastros de objetos + fluxo de controle) para identificar \emph{lifelines}, \emph{messages} e \emph{combined fragments} (\textsf{alt}/\textsf{opt}/\textsf{loop}) de modo inteiramente estático. \textcite{Fauzi2016AST} elimina o XML e vai direto da \textbf{AST} (JavaParser) para \emph{Sequence}, guiado por uma travessia \textbf{DFS pós-ordem} com registro de variáveis, resolução de herança/polimorfismo e marcação de estruturas condicionais/iterativas; a apresentação é automatizada via \textbf{PlantUML}. Já \textcite{Sabir2019MDRE} formalizam o pipeline clássico \textbf{T2M/M2M} em duas fases: da AST para um \textbf{Intermediate Model (XML/EMF)} e, então, do IM (\textit{Intermeadiate Model}) para \textbf{UML} (Classe + \emph{Activity} por operação), com regras de transformação implementadas no ecossistema Eclipse/UML2. O \textbf{híbrido} de \textcite{baidada2016hybrid} desloca a recuperação comportamental para uma IR executável: um \textbf{CFG} orienta a geração de entradas; execuções instrumentadas produzem \emph{traces} filtrados; esses traços são sintetizados como \textbf{Colored Petri Nets (CPN)} e finalmente mapeados para \emph{UML Sequence}, capturando paralelismo e operadores combinados. Em domínio não-Java, \textcite{leatongkam2017generating} mantém a análise estática por \textbf{regras formais de mapeamento} (Fortran OO $\rightarrow$ UML), uma \emph{tree node structure} análoga à AST, e geração de \textbf{XMI} para importação/visualização no ArgoUML, expandindo o ForUML para \emph{Sequence}. Duas linhas recentes aplicam \emph{aprendizado}: \textcite{yang2016condensing} não cria um novo artefato, mas \textbf{condensa} o diagrama de classes com um pipeline \emph{métricas $\rightarrow$ normalização (z-score) $\rightarrow$ k-means $\rightarrow$ under-sampling $\rightarrow$ \emph{ensemble} (Random Forest)}, priorizando classes “importantes” e reduzindo a complexidade visual; e \textcite{siala2024enhancing} introduz \textbf{LLMs} como camada \emph{semântica}: o código é tokenizado/simplificado, traduzido para uma \textbf{representação textual intermediária UML/OCL}, submetida a \emph{model repair} e convertida em diagramas (PlantUML/Graphviz/Modelio). 

\paragraph{RQ5 — Ferramentas/frameworks.}
O ecossistema técnico das abordagens analisadas agrega \emph{parsers}/geradores UML, frameworks MDE e utilitários de visualização/mineração. Em \textcite{zhang2016j2x}, a cadeia J2X apoia-se na \textbf{J2UML} (orquestração), no \textbf{JavaCC} (geração do parser/AST), em \textbf{DOM4J} (manipulação XML) e no próprio \textbf{J2X (DTD/XML)} como IR; os experimentos reportam ambiente Windows 32-bit (3 GB RAM; Core 2 Duo). Em \textcite{yang2016condensing}, para “condensar” diagramas de classes, utilizam-se \textbf{MagicDraw} (recuperação de Classe), \textbf{SDMetrics} (métricas), e \textbf{Random Forest} (classificador), com relatos do ambiente \textit{Windows 7} (64-bit). A \textcite{Fauzi2016AST} (REVUML) integra \textbf{JavaParser} (AST) e \textbf{PlantUML} (renderização do Sequência), dispensando IR XML intermediária. Em \textcite{baidada2016hybrid}, não há ferramenta nominal de ponta a ponta: a coleta se dá por \textbf{instrumentação}/\textbf{JVM}/\textbf{debugger}, a IR comportamental usa \textbf{Colored Petri Nets} (sugerindo uso de \textit{CPN Tools}), e o mapeamento segue \textbf{UML 2}. Em \textcite{leatongkam2017generating}, a extensão da \textbf{ForUML} gera \textbf{XMI (OMG)} para importação no \textbf{ArgoUML} (visualização de Sequência). No framework \textcite{Sabir2019MDRE} (Src2MoF), o gerador baseia-se em \textbf{Eclipse}+\textbf{UML2/EMF} e o \textbf{JavaParser} integra o IMD; ferramentas como \textbf{Papyrus}/\textbf{StarUML}/\textbf{Rational Rose} são citadas apenas para comparação manual, não no pipeline automático. Por fim, \textcite{siala2024enhancing} combina \textbf{AgileUML}/\textbf{OMG MDA} com \textbf{LLMs} (camada semântica) e usa \textbf{Graphviz}, \textbf{PlantUML} e \textbf{Modelio} para materializar \textit{UML/OCL} (fase M2V).

\paragraph{RQ6 — Validação e qualidade prática.}
A avaliação varia de \textbf{estudos de caso pequenos com acurácia estrutural} (classes 96{,}4–100\%, relações 65{,}0–90{,}4) \parencite{zhang2016j2x}, a \textbf{testes sistemáticos} de geração de sequência \parencite{Fauzi2016AST}, e \textbf{comparação especialista} sem métricas quantitativas \parencite{Sabir2019MDRE}. \textcite{yang2016condensing} recorre a \textbf{AUC} e testes estatísticos (Wilcoxon, Cliff’s $\delta$) para condensação de classes. \textcite{baidada2016hybrid} não reporta validação empírica. Em suma, há carência de avaliações \emph{comparativas} com ground truth e métricas padronizadas na maioria dos trabalhos.

\section{metodologia}

Metodologicamente, este trabalho segue uma abordagem de engenharia de software experimental estruturada segundo os princípios de Model-Driven Reverse Engineering (MDRE). Conforme definido por \textcite{Bruneliere2010MoDisco}, um processo MDRE organiza-se em duas fases fundamentais: Model Discovery, responsável por transformar o sistema analisado em um modelo uniforme, e Model Understanding, no qual esse modelo é explorado, transformado e refinado até produzir as representações desejadas \textcite{Bruneliere2010MoDisco}.

Neste estudo, essas duas fases se materializam diretamente na arquitetura do pipeline:

(i) Model Discovery — Construção do modelo intermediário (AST enriquecida)

A fase de descoberta corresponde à extração de um modelo homogêneo a partir do código-fonte. No presente trabalho, essa etapa é realizada por meio da análise sintática e do enriquecimento da AST: o código é convertido em uma coleção estruturada de \texttt{TNode}s contendo informações sintáticas, posicionais, textuais e métricas locais. Esse modelo intermediário funciona como a “vista” uniforme do sistema.

(ii) Model Understanding — Geração de embeddings e interpretação estrutural

A segunda fase do MDRE — compreensão do modelo — é implementada pela geração de embeddings semântico-estruturais e pela construção do grafo reduzido que alimenta a \texttt{CodeGNN}. Essa etapa analisa e transforma o modelo descoberto, produzindo representações vetoriais que codificam padrões sintáticos e semânticos, relações de dependência e proximidade estrutural. Esse processamento equivale às transformações de compreensão e análise previstas na fase de Model Understanding do MoDisco

(iii) Recuperação dos candidatos a casos de uso

Com os embeddings produzidos, o pipeline identifica métodos públicos como candidatos a casos de uso, agrupa-os por similaridade vetorial, e deriva relações estruturais, dependência, inclusão e extensão, a partir do grafo de chamadas. Esta etapa corresponde à aplicação do modelo compreendido para recuperar comportamentos funcionais de mais alto nível.

(iv) Geração do modelo final — diagrama de casos de uso

Por fim, o processo produz um modelo visual final: um diagrama de casos de uso representando funcionalidades reconstruídas e suas relações. Esse artefato sintetiza o entendimento alcançado e cumpre o papel da representação final prevista em MDRE: um modelo abstrato, uniformizado e adequado para documentação, análise e comunicação.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{drawio/comp_flow/fluxo_de_dados.png}
  \caption{Fluxo de dados da metodologia proposta, da análise estática do código-fonte em Python à geração dos embeddings e dos diagramas de casos de uso.}
  \label{fig:fluxo_dados_metodologia}
\end{figure}

\section{Extração do Modelo Intermediário (AST Enriquecida)}
A primeira etapa a consiste em transformar o código-fonte Python em um modelo intermediário, estruturado a partir da \textit{Abstract Syntax Tree} (AST). Esse modelo é representado por instâncias de \texttt{TNode}, que encapsulam tanto informações estruturais quanto textuais extraídas automaticamente do código.

A Figura~\ref{fig:componentes_ast} apresenta a arquitetura responsável por essa etapa, composta por três módulos principais: (i) \textbf{ASTEngine}, que implementa a travessia e a construção dos \textit{TNodes}; (ii) \textbf{PassPlugins}, que enriquece cada nó com metadados adicionais; e (iii) \textbf{ASTApi}, que fornece a fachada de alto nível para execução da análise em repositórios completos.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{drawio/comp_flow/diagrama_de_componentes_AST.png}
    \caption{Arquitetura da etapa de extração da AST enriquecida.}
    \label{fig:componentes_ast}
\end{figure}

\subsection{Arquitetura da Extração}

A etapa de \emph{Analyse Path} é realizada no módulo \texttt{service}, que percorre recursivamente o diretório fornecido, identifica os arquivos \texttt{.py}, lê seu conteúdo e inicializa o contexto de análise (\texttt{Ctx}) para cada arquivo. Em seguida, ocorre a fase de \emph{Source Code Parsing}, na qual o conteúdo do arquivo é convertido para uma AST nativa do Python (\texttt{ast.parse}) e imediatamente processado pelo motor \textbf{ASTEngine} através da função \texttt{walk\_module}. Essa etapa não apenas interpreta o código, mas também constrói o modelo intermediário, criando instâncias de \texttt{TNode} que representam cada elemento sintático do arquivo.

O motor \textbf{ASTEngine} conduz uma travessia controlada da AST, seguindo estratégias configuráveis (\textit{recursive pre-order}, \textit{post-order}, \textit{iterative}, \textit{BFS}). Abordagens clássicas voltadas à recuperação de comportamento e de fluxos de execução tendem a empregar \emph{depth-first search} em pós-ordem (\emph{DFS post-order}). No contexto da ferramenta REVUML, por exemplo, \textcite{Fauzi2016AST} argumentam que a travessia pós-ordem é mais adequada para preservar a sequência das instruções no código e, assim, apoiar a geração de diagramas de sequência e a reconstrução da
ordem de execução.

Por outro lado, trabalhos recentes focados em representação vetorial de código e em modelos neurais para compreensão de programas têm privilegiado variantes de pré-ordem. Estudos como o de \textcite{LiangHuang2024ASTLLM} mostram que a linearização da AST em pré-ordem (especialmente na forma \emph{reverse pre-order}) produz sequências mais estáveis e informativas para técnicas de \emph{tree positional encoding} e embeddings estruturais, levando a ganhos consistentes em tarefas de classificação, sumarização e \emph{clustering} de código.

Durante essa travessia, são criados dois elementos fundamentais: o contexto de análise (\texttt{Ctx}) e os nós enriquecidos (\texttt{TNode}). O \texttt{Ctx} mantém o estado global do arquivo (linhas de código, comentários, pilhas de classes e funções), enquanto cada \texttt{TNode} encapsula o nó original da AST e recebe metadados estruturais (profundidade na AST, relações pai–filho, posição no arquivo) e textuais.

O enriquecimento dos nós é realizado pelos \textbf{passes} registrados em \texttt{PassPlugins}, organizados em fases (\textit{PRE}, \textit{ENRICH}, \textit{POST}) e ordenados por dependências declaradas via \texttt{PassRegistry}. Cada plugin preenche campos específicos do \texttt{TNode}, como \textit{PathInfo}, \textit{NamesVisibility}, \textit{MethodKind}, \textit{IOSignature}, \textit{ClassKind}, \textit{DocsComments} e \textit{CallGraph}.

Ao final, o módulo \texttt{service} agrega todos os \texttt{TNode}s enriquecidos em uma instância \texttt{AnalysisResult}, que representa o modelo intermediário sobre oqual as etapas posteriores de geração de embeddings e recuperação de casos de uso serão executadas.


\section{Geração de Embeddings Semântico-Estruturais (AST + GNN)}

A segunda etapa parte do modelo intermediário produzido pela análise de AST, composto por instâncias de \texttt{TNode} enriquecidas com metadados estruturais e textuais. Cada \texttt{TNode} é transformado em um vetor numérico que combina informações sobre o papel daquele elemento no código (tipo de nó, visibilidade, tipo de classe e método, grau de participação no grafo de chamadas, profundidade, entre outros) com uma representação densa do vocabulário associado (nomes, docstrings e comentários).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{drawio/comp_flow/diagrama_de_componentes_embeddings.png}
    \caption{Arquitetura da etapa de geração de embeddings.}
    \label{fig:componentes_embeddings}
\end{figure}

\subsection{Codificação estrutural e textual do nó}

Seguindo a diretriz de \textcite{tonella2007reverse}, que recomenda descartar detalhes internos dos corpos de métodos por não contribuírem para a recuperação de modelos arquiteturais, o pipeline reduz a AST a um subconjunto de nós estruturalmente relevantes. Essa redução é implementada pelo \textit{pass} \texttt{core\_nodes}, que marca em cada \texttt{TNode} um indicador \texttt{is\_core} e um rótulo \texttt{core\_kind}, identificando apenas escopos (\texttt{Module}, \texttt{ClassDef}, \texttt{FunctionDef}, \texttt{AsyncFunctionDef}) e operações essenciais (\texttt{assign}, \texttt{call}, \texttt{return}, \texttt{raise}) como nós centrais.

A codificação estrutural é realizada pela função \texttt{structural\_features}, que mapeia cada \texttt{TNode} para um vetor fixo de dimensão \texttt{STRUCT\_DIM}. Esse vetor é composto por blocos \emph{one-hot} para categorias discretas (tipo reduzido do nó na AST, visibilidade, \emph{class kind}, \emph{method kind} e \emph{core\_kind}) e por um bloco contínuo de \emph{flags} numéricas. Entre essas \emph{flags} estão: número de parâmetros, profundidade normalizada na AST, número de exceções lançadas, quantidade de classes base e decoradores, métricas de \emph{fan-in} e \emph{fan-out} no grafo de chamadas e a posição do arquivo no diretório (\emph{file depth}).

Em paralelo, a codificação textual utiliza um encoder semântico baseado em \emph{Sentence-BERT} (\texttt{all-MiniLM-L6-v2}). Para cada nó, a função \texttt{concat\_text\_from\_tnode} constrói uma descrição textual em três níveis: (i) informações de nomeação e contexto, incluindo o nome qualificado (\texttt{qname}), os \emph{tokens} do identificador (\texttt{name\_tokens}), o estilo de nomeação (\textit{snake\_case}, \textit{camelCase}, \textit{PascalCase}) e os nomes de pacote e módulo; (ii) documentação associada, composta por \textit{docstrings}, comentários de cabeçalho e comentários inline; e (iii) indícios de comportamento extraídos automaticamente, como expressões de retorno, anotações de tipo e chamadas locais a outros símbolos do mesmo módulo.

Neste trabalho, chamamos de \emph{tokens do identificador} as subpalavras obtidas a partir do nome original. Por exemplo, os identificadores \texttt{getUserName} e \texttt{user\_name} são decompostos, respectivamente, em \texttt{[get, user, name]} e \texttt{[user, name]}. Essa decomposição separa unidades semânticas menores, permitindo que o encoder textual explore melhor o significado dos nomes do código.

A inclusão explícita dos \emph{tokens} de identificadores e do estilo de nomeação é inspirada em resultados recentes que mostram que a decomposição semântica de nomes de variáveis e métodos melhora a capacidade do modelo de inferir o papel estrutural e funcional dos elementos do código \textcite{Ahmad2020TransformerCodeSummarization}. No presente trabalho, o texto consolidado é truncado em até 1024 caracteres e codificado pelo \texttt{SBERTTextEncoder}, produzindo um embedding semântico denso de dimensão fixa (\texttt{TEXT\_EMB\_DIM}).

\subsection{Encoder semântico e combinação estrutural–textual}

Uma vez definidas as \emph{features} estruturais e o texto consolidado de cada nó, a etapa seguinte consiste em projetar essas informações em um espaço vetorial denso adequado para o processamento por grafos. Essa projeção é realizada em duas camadas: (i) um encoder semântico baseado em \textit{Sentence-BERT} para o texto associado ao nó e (ii) um \textit{Multi-Layer Perceptron} (MLP) que combina o vetor estrutural e o embedding textual em uma única representação compacta.

O encoder semântico é encapsulado na classe \texttt{SBERTTextEncoder}, que carrega um modelo pré-treinado \texttt{SentenceTransformer} (\texttt{all-MiniLM-L6-v2}) e expõe um método \texttt{encode} responsável por mapear uma descrição textual arbitrária para um vetor denso de dimensão fixa (\texttt{TEXT\_EMB\_DIM}). Para evitar o custo de inicialização repetida do modelo, o encoder é instanciado uma única vez, por meio de um padrão de projeto do tipo \textit{singleton} (\texttt{get\_text\_encoder}), e reutilizado em toda a análise. A função \texttt{text\_features(t)} aplica esse encoder ao texto gerado por \texttt{concat\_text\_from\_tnode}, produzindo um embedding semântico \(x_t \in \mathbb{R}^{\texttt{TEXT\_EMB\_DIM}}\).

Esses dois vetores são combinados pelo módulo \texttt{TNodeMLPEncoder}, implementado como uma rede MLP de duas camadas. Esse encoder recebe como entrada a concatenação do vetor estrutural e do embedding textual e aplica primeiro uma normalização de camada (\texttt{LayerNorm}) sobre o vetor conjunto, seguida de uma projeção não linear:

\[
z_t = [\, s_t \,\|\, x_t \,], \qquad
\hat{z}_t = \text{LayerNorm}(z_t), \qquad
h_t = W_2 \,\sigma(W_1 \hat{z}_t + b_1) + b_2,
\]

em que \(s_t \in \mathbb{R}^{\texttt{STRUCT\_DIM}}\) representa as \emph{features} estruturais do nó \(t\), \(x_t \in \mathbb{R}^{\texttt{TEXT\_EMB\_DIM}}\) é o embedding semântico produzido pelo SBERT, \([\,\cdot \,\|\, \cdot\,]\) indica concatenação, \(\sigma\) é a função de ativação ReLU e \(h_t \in \mathbb{R}^D\) é o vetor resultante com dimensão de saída \texttt{out\_dim}. 

A função \texttt{make\_tnode\_encoder} instancia o \texttt{TNodeMLPEncoder} com os valores corretos de \texttt{STRUCT\_DIM} e \texttt{TEXT\_EMB\_DIM}, garantindo que a rede seja compatível com o encoder textual carregado. Por fim, a função \texttt{encode\_tnode} aplica, para cada \texttt{TNode}, o pipeline completo de codificação estrutural e textual, retornando o vetor \(h_t\) já no dispositivo (\textit{CPU} ou \textit{GPU}) em que a GNN será executada.

Ao aplicar \texttt{encode\_tnode} a todos os nós de um arquivo, a função \texttt{build\_node\_feature\_matrix} empilha esses vetores em uma matriz de \emph{features}
\[
X \in \mathbb{R}^{N \times D},
\]
em que \(N\) é o número de nós considerados (nós \emph{core}) e \(D = \texttt{out\_dim}\) é a dimensão de saída do encoder. Essa matriz representa o ponto de partida para a etapa de convolução em grafos, na qual as informações semânticas e estruturais de cada nó serão refinadas em função da vizinhança na AST reduzida.

\subsection{Construção do grafo e pesos gaussianos}

Sobre esse conjunto de nós é construído um grafo derivado da AST reduzida. A função \texttt{build\_core\_edge\_index} seleciona apenas os nós marcados como \emph{core} (escopos, atribuições, chamadas, retornos e lançamentos de exceção) e conecta cada nó core \(c_i\) ao ancestral core mais próximo \(c_j\) na árvore sintática. Para cada aresta candidata \(e = (i,j)\), são calculadas duas medidas posicionais: (i) o \emph{gap} de nós intermediários ignorados entre o filho e o ancestral e (ii) a diferença de profundidade entre ambos na AST (\emph{depth}).

Seja \(n_{\text{interm}}(i,j)\) o número de nós não--core entre \(c_i\) e \(c_j\), e \(\text{depth}(\cdot)\) a profundidade do nó na árvore. As medidas brutas são definidas como:
\[
\mathrm{gap}_{ij} = n_{\text{interm}}(i,j), 
\qquad
\Delta \mathrm{depth}_{ij} = \bigl|\,\text{depth}(c_i) - \text{depth}(c_j)\,\bigr|.
\]

No código, ambas são normalizadas por uma transformação logarítmica estável, usando \texttt{log1p}:
\[
\tilde{g}_{ij} = \log(1 + \mathrm{gap}_{ij}), 
\qquad
\tilde{d}_{ij} = \log\bigl(1 + \Delta \mathrm{depth}_{ij}\bigr).
\]

Esses dois sinais alimentam a função \texttt{hibrid\_weight\_gaussian}, que aplica funções de similaridade gaussiana separadas para \(\tilde{g}_{ij}\) e \(\tilde{d}_{ij}\). Dados os hiperparâmetros \(\sigma_{\text{gap}}\) e \(\sigma_{\text{depth}}\), as similaridades são definidas por:
\[
\mathrm{sim}_{\text{gap}}(i,j)
=
\exp\!\left(
  - \frac{\tilde{g}_{ij}^{2}}{2\sigma_{\text{gap}}^{2}}
\right),
\qquad
\mathrm{sim}_{\text{depth}}(i,j)
=
\exp\!\left(
  - \frac{\tilde{d}_{ij}^{2}}{2\sigma_{\text{depth}}^{2}}
\right).
\]
Esses dois sinais alimentam a função \texttt{hibrid\_weight\_gaussian}, que aplica funções de similaridade gaussiana separadas para \(\tilde{g}_{ij}\) e \(\tilde{d}_{ij}\). Dados os hiperparâmetros \(\sigma_{\text{gap}}\) e \(\sigma_{\text{depth}}\), as similaridades são definidas por:
\[
\mathrm{sim}_{\text{gap}}(i,j)
=
\exp\!\left(
  - \frac{\tilde{g}_{ij}^{2}}{2\sigma_{\text{gap}}^{2}}
\right),
\qquad
\mathrm{sim}_{\text{depth}}(i,j)
=
\exp\!\left(
  - \frac{\tilde{d}_{ij}^{2}}{2\sigma_{\text{depth}}^{2}}
\right).
\]

Nessa formulação, \(\sigma_{\text{gap}}\) e \(\sigma_{\text{depth}}\) controlam a \emph{escala} da vizinhança considerada relevante em cada dimensão posicional. Valores menores de \(\sigma\) tornam o kernel mais “concentrado”: pequenas variações em \(\tilde{g}_{ij}\) ou \(\tilde{d}_{ij}\) já reduzem significativamente a similaridade, privilegiando arestas muito locais na AST. Valores maiores de \(\sigma\) produzem um decaimento mais suave, permitindo que nós separados por gaps ou diferenças de profundidade maiores ainda contribuam com pesos não desprezíveis na propagação de mensagens. Na prática, esses hiperparâmetros controlam o quão sensível a GNN é à distância estrutural entre os nós.

Os pesos finais utilizados pela GNN em cada aresta \(e=(i,j)\) são obtidos combinando essas duas similaridades. O código oferece dois modos: combinação por produto,
\[
w_{ij} = \mathrm{sim}_{\text{gap}}(i,j)\,\cdot\,\mathrm{sim}_{\text{depth}}(i,j),
\]
ou por média aritmética,
\[
w_{ij} = \frac{\mathrm{sim}_{\text{gap}}(i,j) + \mathrm{sim}_{\text{depth}}(i,j)}{2}.
\]

A escolha do modo de combinação impacta a forma como a distância estrutural é interpretada pela GNN. No modo \texttt{product}, o peso só permanece alto quando a aresta é simultaneamente próxima em ambas as dimensões (gap e profundidade); uma similaridade baixa em qualquer uma delas reduz o peso total, aproximando o comportamento de um operador lógico ``E'' suave.  Já no modo \texttt{mean}, uma similaridade alta em apenas uma das dimensões é suficiente para manter um peso intermediário, o que corresponde a uma combinação mais permissiva, semelhante a um ``OU'' suave entre as duas noções de proximidade.

O vetor de pesos resultante,
\[
\mathbf{w} = \bigl(w_{ij}\bigr)_{(i,j)\in E} \in \mathbb{R}^{|E|},
\]
atua como um viés posicional suave no grafo: arestas que conectam nós estruturalmente próximos na AST (pequenos \(\tilde{g}_{ij}\) e \(\tilde{d}_{ij}\)) recebem pesos mais altos, enquanto conexões entre regiões distantes do código são atenuadas. Esses pesos são passados diretamente como \emph{edge\_weight} para a \texttt{CodeGNN} durante a propagação de mensagens.

\subsection{GNN e níveis de embedding}

A etapa final de codificação é realizada pelo modelo \texttt{CodeGNN}, composto por duas camadas de Graph Convolutional Network (\texttt{SimpleGCNLayer}) com normalização simétrica do grau e inclusão explícita de auto-laços, seguidas de uma etapa de \emph{global\ attention\ pooling}. A GNN recebe a matriz (X), o grafo de arestas (\texttt{edge\_index}) e os pesos gaussianos (\texttt{edge\_weight}), produzindo embeddings refinados para cada nó da AST e um embedding agregado para o arquivo.

Cada camada \texttt{SimpleGCNLayer} implementa a operação padrão de uma Graph Convolutional Network (GCN) com normalização simétrica do grau. Seja \(A\) a matriz de adjacência do grafo reduzido e \(X\) a matriz de features de entrada. No código, são adicionados auto-laços (\emph{self-loops}) a todos os nós, o que corresponde a trabalhar com \(\hat{A} = A + I\). A matriz de graus é então recalculada como \(\hat{D}_{ii} = \sum_j \hat{A}_{ij}\) e a atualização de cada camada segue a forma:
\[
H^{(l+1)} = \sigma\bigl(\hat{D}^{-1/2} \hat{A}\, \hat{D}^{-1/2} H^{(l)} W^{(l)}\bigr),
\]
onde \(H^{(0)} = X\), \(W^{(l)}\) são as matrizes de pesos aprendidas e \(\sigma\) é a ativação ReLU. A inclusão de auto-laços garante que cada nó preserve e propague também as suas próprias features, e não apenas a informação agregada dos vizinhos.

Para obter um embedding em nível de arquivo, o modelo \texttt{CodeGNN} aplica um esquema de \emph{global attention pooling}. Dado o conjunto de embeddings de nós \(h_i\), um pequeno MLP (\texttt{attn\_gate}) calcula um escalar de importância \(s_i = a(h_i)\) para cada nó. Esses escores são normalizados por \textit{softmax},
\[
\alpha_i = \frac{\exp(s_i)}{\sum_j \exp(s_j)},
\]
e o embedding do grafo é calculado como uma média ponderada:
\[
g = \sum_i \alpha_i\, h_i.
\]
Dessa forma, nós considerados mais relevantes pelo mecanismo de atenção contribuem mais para o vetor final do arquivo do que nós estruturalmente periféricos, em contraste com um \emph{pooling} puramente uniforme (por exemplo, média simples).

Em seguida, a função \texttt{encode\_repository\_with\_gnn} aplica esse processo a todos os arquivos Python do repositório, gerando (i) um mapa de nós \texttt{TNode} por arquivo, (ii) tensores com embeddings de nós, (iii) embeddings de grafo por arquivo e (iv) um embedding do repositório, obtido pela média dos vetores de arquivo. A API de alto nível \texttt{run\_gnn\_repository} encapsula esse fluxo e expõe os hiperparâmetros principais (dimensão de saída, tamanho máximo de texto, sigmas das gaussianas e modo de combinação), permitindo reproduzir o processo de forma configurável nos experimentos.

Do ponto de vista conceitual, o pipeline produz três níveis de representação vetorial:

\begin{itemize}
    \item \textbf{embeddings de nó}, que capturam o papel semântico e estrutural de cada elemento do código;
    \item \textbf{embeddings de arquivo}, que sintetizam o comportamento e a organização de cada unidade de compilação;
    \item \textbf{embedding de repositório}, que resume a visão global do sistema analisado.
\end{itemize}

Esses embeddings são reutilizados na etapa seguinte para selecionar candidatos a casos de uso, agrupar métodos semanticamente semelhantes e derivar relações de dependência, inclusão e extensão entre funcionalidades.


\section{Recuperação e Agrupamento de Casos de Uso}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{drawio/comp_flow/diagrama_de_componentes_usecases.png}
    \caption{Arquitetura da etapa de recuperação e agrupamento de casos de uso.}
    \label{fig:componentes_usecases}
\end{figure}

A terceira etapa do pipeline visa identificar, a partir dos embeddings gerados pela GNN, os casos de uso implementados no sistema. Essa etapa é dividida em duas fases principais: (i) a recuperação inicial de candidatos a casos de uso, baseada em heurísticas estruturais aplicadas à AST enriquecida; e (ii) o agrupamento desses candidatos por similaridade semântica, utilizando algoritmos de \emph{clustering} no espaço vetorial dos embeddings.

A identificação inicial dos casos de uso parte diretamente da AST enriquecida, na qual cada nó (\texttt{TNode}) contém informações estruturais como qualificador de visibilidade, nome qualificado (\texttt{qname}), classe proprietária, métodos chamados e outros metadados adicionais. Seguindo a heurística clássica apresentada por \textcite{pereira2011recovering}, cada método público é tratado como um \emph{caso de uso básico}, pois representa uma operação exposta por uma classe e, portanto, uma funcionalidade observável do sistema. 

Essa regra simplifica e padroniza a detecção inicial dos casos de uso, garantindo que apenas operações acessíveis externamente sejam consideradas candidatas.

A função \texttt{collect\_uc\_candidates} percorre todos os \texttt{TNode}s da AST e realiza três tarefas principais. Em primeiro lugar, \emph{indexa as classes encontradas}: cada nó que representa uma classe é armazenado em um índice por nome qualificado, permitindo recuperar informações como o tipo da classe (\texttt{class\_kind}), suas classes-base (herança) e demais metadados úteis para a posterior inferência de relações entre casos de uso. Em seguida, a função \emph{filtra os métodos públicos}, convertendo em candidatos apenas os nós configurados como métodos públicos.

Por fim, para cada método público é construído um objeto \texttt{UseCaseCandidate}, que reúne:

\begin{itemize}
  \item um identificador estável (\texttt{uc\_id}), normalmente o nome qualificado \texttt{Classe.método};
  \item a posição do nó original na lista de \texttt{TNode}s (\texttt{tnode\_index}) e uma referência direta ao \texttt{TNode} correspondente;
  \item a classe proprietária (\texttt{owner\_class\_qname}), seu tipo (\texttt{owner\_class\_kind}) e suas classes-base (\texttt{owner\_base\_classes});
  \item a lista de métodos chamados (\texttt{called\_qnames}), indispensável para recuperar dependências entre casos de uso;
  \item a profundidade do arquivo no qual o método se encontra (\texttt{file\_depth}), um atributo estrutural que indica o quão “superficial” ou “interno” é o componente dentro do repositório analisado.
\end{itemize}

O resultado é uma lista estruturada de candidatos a casos de uso, pronta para ser utilizada nas etapas subsequentes do pipeline: geração de embeddings, \emph{clustering}, classificação de papéis e construção das relações entre casos de uso (\emph{include}, \emph{extend}, generalização e dependência).

\subsection{Agrupamento e Seleção de Casos de Uso}

Uma vez identificados os candidatos a casos de uso, cada \texttt{UseCaseCandidate} é projetado para o espaço vetorial gerado pela GNN. A função \texttt{extract\_uc\_embeddings} seleciona, a partir da matriz de embeddings de nós produzida pelo modelo (\texttt{node\_embs}), apenas as linhas correspondentes aos índices dos candidatos e aplica normalização L2 linha a linha.


\paragraph{Escolha automática do número de clusters.}
Quando o número de clusters não é fornecido explicitamente pelo usuário, o sistema aplica uma rotina inspirada no método Kernel k-MACE de \textcite{rahman2018kernel}. A função \texttt{choose\_m\_and\_sigma\_with\_kmace\_like} avalia uma grade de valores para \(m\) (número de clusters) e \(\sigma\) (largura do kernel gaussiano) e, para cada combinação, executa \texttt{kernel\_kmeans\_labels\_and\_compactness}. Essa rotina calcula: (i) o \emph{data error} médio \(Y_{sm}\), medido como a distância média de cada ponto ao centro do seu cluster no espaço do kernel; (ii) a variância dessas distâncias \(\mathrm{Var}_{sm}\); e (iii) uma aproximação da matriz de covariância de cada cluster no espaço original.

Com esses valores, é construído um escore composto
\[
Z_{sm} \approx Y_{sm} + \beta_N \sqrt{\mathrm{Var}_{sm}} + \lambda m,
\]
no qual o primeiro termo favorece configurações mais compactas, o segundo controla a variabilidade intra\-cluster (proxy do \emph{bound} de generalização do Kernel k-MACE) e o terceiro penaliza soluções com muitos clusters. Na implementação, esses termos são mapeados diretamente para o código da seguinte forma: \(Y_{sm}\) corresponde à variável \texttt{y\_sm}, \(\mathrm{Var}_{sm}\) à variável \texttt{var\_sm}, \(\beta_N\) ao parâmetro \texttt{beta\_N} e \(\lambda\) ao parâmetro \texttt{penalty\_lambda} da função \texttt{choose\_m\_and\_sigma\_with\_kmace\_like}. A combinação \((m,\sigma)\) que minimiza \texttt{z\_sm} (implementação de \(Z_{sm}\)) é escolhida como configuração final de agrupamento.


\paragraph{Clustering com \emph{k}-means.}
Definidos o número de clusters e, quando necessário, o valor de \(\sigma\), os candidatos são agrupados pela função \texttt{run\_kmeans\_uc}. Esse módulo permite duas variantes: (i) \emph{k}-means linear clássico, usando a implementação do \textit{scikit-learn} com distância euclidiana; ou (ii) \emph{kernel} \emph{k}-means, que opera sobre uma matriz de similaridade gaussiana \(K\), calculada a partir dos embeddings com o kernel
\(
K_{ij} = \exp\bigl(-\lVert x_i - x_j\rVert^2 / (2\sigma^2)\bigr).
\)
Na prática, o pipeline utiliza a versão com kernel, aproximando o comportamento do Kernel k-MACE ao agrupar os casos de uso em regiões mais coesas no espaço de features não linear.

O resultado desse processo é encapsulado na estrutura \texttt{UcClusteringResult}, que armazena: (i) o vetor de rótulos de cluster para cada candidato; (ii) os centroides dos clusters (no espaço original dos embeddings); e (iii) a lista de membros por cluster, juntamente com um representante selecionado para cada grupo.

\paragraph{Seleção de casos de uso representativos.}
A escolha dos casos de uso que serão destacados no diagrama parte diretamente dos clusters obtidos. Para cada cluster, a função \texttt{run\_kmeans\_uc} seleciona um único representante (\emph{focus use case}) em duas etapas.

Na primeira etapa, o algoritmo tenta restringir a escolha a métodos considerados ``mais abstratos''. Para isso, a função \texttt{is\_abstract\_like} verifica se o candidato está associado a uma classe marcada como \texttt{abstract} ou \texttt{protocol}; quando existirem tais métodos dentro do cluster, apenas eles são considerados como candidatos a representante. Caso contrário, todos os membros do cluster são elegíveis.

Na segunda etapa, os candidatos elegíveis são ordenados por um escore que combina centralidade semântica e profundidade estrutural. Seja \(x_i \in \mathbb{R}^D\) o embedding do candidato \(i\) no cluster \(k\) e \(\mu_k\) o centróide desse cluster (média dos embeddings no espaço original). Para cada candidato é calculado:
\[
\mathrm{score}(i) = \lVert x_i - \mu_k \rVert_2 \;+\; \alpha \cdot \mathrm{depth\_norm}(i),
\]
onde o primeiro termo mede a distância euclidiana do embedding ao centróide (centralidade no espaço semântico) e o segundo termo penaliza candidatos definidos em arquivos mais profundos na árvore de diretórios. Na implementação, a profundidade normalizada \(\mathrm{depth\_norm}(i)\) é obtida dividindo-se o \texttt{file\_depth} do candidato pelo valor máximo observado no conjunto (\texttt{max\_depth}), produzindo um valor em \([0,1]\), e o peso \(\alpha\) corresponde ao parâmetro \texttt{depth\_weight} da função \texttt{run\_kmeans\_uc}. O representante do cluster é definido como o candidato com menor valor de \(\mathrm{score}(i)\).

Com isso, cada cluster passa a ser representado por um caso de uso que, ao mesmo tempo, (i) é próximo ao centro do grupo no espaço vetorial aprendido pela GNN e (ii) tende a residir em arquivos mais ``superficiais'' da arquitetura, em linha com a hipótese de que casos de uso principais são expostos em camadas menos internas do sistema.

\paragraph{Relações estruturais entre casos de uso.}
Após o agrupamento dos candidatos, o pipeline reconstrói automaticamente as relações estruturais entre os casos de uso. Três tipos principais de relações são extraídos:

\begin{itemize}
    \item \textbf{Generalização} — Quando um método de uma subclasse sobrescreve um método definido na classe-base, cria-se uma relação \texttt{superclass.m} $\rightarrow$ \texttt{subclass.m}. Essa inferência é realizada pela função \texttt{build\_uc\_generalization\_edges}, que agrupa os candidatos por classe proprietária e verifica, para cada subclasse, se alguma de suas classes-base define um método com o mesmo nome.

    \item \textbf{Dependências} — A partir das chamadas registradas em \texttt{called\_qnames}, a função \texttt{build\_dependency\_edges} constrói arestas entre casos de uso, representando que um método público depende da execução de outro (Arquivo: \texttt{edges.py}).

    \item \textbf{Estereótipos de dependência} — Cada dependência é classificada como \emph{include}, \emph{extend} ou \emph{dependency}. A função \texttt{build\_dependency\_edge\_infos} agrega informações sobre chamadas guardadas e não guardadas, enquanto \texttt{classify\_edge} aplica heurísticas inspiradas na semântica UML (por exemplo, chamadas reutilizadas no fluxo principal sugerem \emph{include}). A função \texttt{group\_edges\_by\_kind} organiza essas relações em dependências semânticas e genéricas, permitindo sua renderização diferenciada no diagrama final.
\end{itemize}

\paragraph{Cálculo de métricas e papéis.}
Com as relações de dependência estabelecidas, o pipeline calcula métricas estruturais para cada caso de uso por meio da função \texttt{compute\_uc\_metrics}. Essa função faz o cálculo de \emph{fan-in} e \emph{fan-out} (\texttt{compute\_fan\_in\_out}), que quantificam, respectivamente, quantas vezes um caso de uso é chamado por outros e quantas vezes ele chama outros casos de uso.

A partir dessas métricas, a função \texttt{classify\_uc\_role} atribui um papel estrutural a cada caso de uso:

\begin{itemize}
    \item \textbf{entry} — casos de uso com \texttt{fan-in = 0} e \texttt{fan-out > 0}, funcionando como pontos de entrada lógicos (operações que iniciam fluxos);
    \item \textbf{bridge} — casos de uso com \texttt{fan-in > 0} e \texttt{fan-out > 0}, atuando como intermediários que encadeiam funcionalidades;
    \item \textbf{helper} — casos de uso com \texttt{fan-in > 0} e \texttt{fan-out = 0}, caracterizando operações auxiliares frequentemente reutilizadas;
    \item \textbf{isolated} — casos de uso sem dependências relevantes, indicando funções desconectadas do fluxo principal.
\end{itemize}

\paragraph{Inferência de \textit{include} e \textit{extend}.}
A partir das métricas estruturais calculadas, em particular dos valores de \emph{fan-in} e \emph{fan-out}, o pipeline realiza a classificação semântica das dependências entre casos de uso. Essa etapa utiliza os mesmos identificadores presentes em \texttt{metrics} para distinguir quando uma chamada representa reutilização obrigatória, comportamento alternativo ou apenas uma dependência genérica.

Primeiro, a função \texttt{build\_dependency\_edge\_infos} agrega, para cada par 
\(
(\mathrm{UC}_{src}, \mathrm{UC}_{dst})
\),
o número total de chamadas observadas, bem como quantas são condicionais (\texttt{guarded\_calls}) e quantas ocorrem no fluxo principal (\texttt{unguarded\_calls}). Esses valores descrevem o padrão de ativação de cada dependência.

Em seguida, a função \texttt{classify\_edge} utiliza o \emph{fan-in} do destino, recuperado diretamente do dicionário \texttt{metrics}, em conjunto com o padrão de guarda das chamadas para rotular semanticamente cada aresta. Um caso de uso \texttt{dst} é classificado como alvo de um \texttt{<<include>>} quando apresenta reutilização
\[
\mathrm{fan\_in}_{dst} \ge 1
\]
e todas as chamadas que o ativam ocorrem de forma incondicional (\texttt{guarded\_calls} = 0 e \texttt{unguarded\_calls} > 0), caracterizando um subfluxo obrigatório.

Por outro lado, o rótulo \texttt{<<extend>>} é atribuído quando todas as chamadas são condicionais (\texttt{guarded\_calls} > 0 e \texttt{unguarded\_calls} = 0), indicando um comportamento opcional ou alternativo que só é executado mediante uma condição específica no código.

Nos casos em que nenhuma dessas condições é satisfeita, a relação é tratada como \texttt{dependency}. Finalmente, a função \texttt{group\_edges\_by\_kind} organiza as arestas classificadas em dependências semânticas (\texttt{include} e \texttt{extend}) e dependências genéricas, permitindo que o diagrama final reflita de maneira diferenciada os três tipos de vínculos.

\subsection{Geração do diagrama de casos de uso.}
Com todas as métricas calculadas e as relações (\emph{dependency}, \emph{include}, \emph{extend} e generalização) devidamente classificadas, o pipeline constrói a especificação final do diagrama. A função \texttt{build\_filtered\_diagram\_spec} seleciona quais casos de uso serão exibidos, garantindo que todos os representantes de cluster apareçam no diagrama e incluindo automaticamente seus vizinhos estruturais quando necessário. Por fim, \texttt{export\_diagram\_spec\_to\_puml} gera o arquivo PlantUML contendo os nós, estereótipos e arestas resultantes da análise, enquanto o módulo \texttt{generate\_usecase\_diagram} encapsula esse processo de ponta a ponta, produzindo a imagem final do diagrama. Assim, o modelo sintetiza em uma única visualização os principais casos de uso identificados, seus papéis estruturais e suas relações funcionais.

\subsubsection*{Parâmetros do pipeline de análise de casos de uso}

A Tabela~\ref{tab:params-usecase} descreve os principais parâmetros utilizados na execução do pipeline  (\texttt{run\_usecase\_analysis}), incluindo configurações de extração da AST, geração de embeddings, pesos gaussianos do grafo reduzido, clustering e seleção de representantes.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{p{3cm} p{11cm}}
\hline
\textbf{Parâmetro} & \textbf{Descrição} \\
\hline

\texttt{repo\_path} &
Diretório raiz do repositório Python a ser analisado. Todos os arquivos \texttt{.py} são processados recursivamente. \\

\texttt{out\_dir} &
Diretório onde serão salvos o arquivo \texttt{.puml}, sua renderização e artefatos auxiliares. \\

\texttt{file\_name} &
Nome do arquivo PlantUML a ser gerado. \\

\texttt{plantuml\_jar} &
Caminho para o \texttt{plantuml.jar}, usado para renderizar o diagrama em imagem. \\

\hline
\multicolumn{2}{l}{\textbf{Parâmetros da GNN e extração da AST}} \\
\hline

\texttt{strategy} &
Estratégia de travessia da AST (ex.: \texttt{recursive\_pre}), controlando a ordem de visitação dos nós. \\

\texttt{out\_dim} &
Dimensão dos embeddings produzidos pela GNN para cada nó e para cada arquivo. \\

\texttt{max\_chars} &
Número máximo de caracteres do trecho de código usado como entrada textual para o encoder. \\

\hline
\multicolumn{2}{l}{\textbf{Pesos gaussianos do grafo reduzido}} \\
\hline

\texttt{sigma\_gap} &
Largura da gaussiana aplicada à medida de separação entre nós ``core'' (gap). 
Controla o quanto a similaridade decai com a distância estrutural. \\

\texttt{sigma\_depth} &
Largura da gaussiana aplicada à diferença de profundidade na árvore syntática. \\

\texttt{mode} &
Modo de combinação das duas gaussianas (\textit{gap} e \textit{depth}), 
por exemplo \texttt{"product"} ou \texttt{"mean"}. \\

\hline
\multicolumn{2}{l}{\textbf{Configurações do clustering (k-means / kernel k-means)}} \\
\hline

\texttt{n\_clusters} &
Número de clusters desejados. Se \texttt{None}, o pipeline aplica a rotina 
kmace-like para escolher automaticamente o melhor valor de \(m\). \\

\texttt{use\_kernel} &
Se \texttt{True}, utiliza Kernel k-means com kernel gaussiano; caso contrário, 
usa k-means euclidiano clássico. \\

\texttt{sigma} &
Largura da gaussiana usada no Kernel k-means. \\

\texttt{beta\_N} &
Peso do termo de variância no escore \(Z_{sm}\) da rotina kmace-like, 
controlando a penalização de clusters muito difusos. \\

\texttt{penalty\_lambda} &
Coeficiente \(\lambda\) que penaliza soluções com muitos clusters no cálculo de \(Z_{sm}\). \\

\hline
\multicolumn{2}{l}{\textbf{Seleção dos representantes dos clusters}} \\
\hline

\texttt{depth\_weight} &
Peso \(\alpha\) do termo de profundidade de arquivo no escore usado para seleção do representante.
Valores maiores priorizam casos de uso definidos em arquivos mais ``superficiais'' na estrutura do repositório. \\

\hline
\end{tabular}
\caption{Descrição dos principais parâmetros do pipeline de análise de casos de uso.}
\label{tab:params-usecase}
\end{table}

\section{Escolha dos repositórios}

Para a análise foram escolhidos três repositórios independentes, dois de David Beazley e um de Brandon Rhodes, duas referências em linguagem Python. Os repositórios de David Beazley possuem documentação completa no próprio repositório, facilitando a compreensão do software construído. Já o repositório de Brandon Rhodes não contém documentação, mas seu conteúdo é a adaptação do jogo \emph{Colossal Cave Adventure} de Fortran para Python.

\subsection{Colossal Cave Adventure}
Este trabalho utiliza como base uma reimplementação de \textcite{rhodes_adventure_py} em Python 3, que preserva o jogo original de Crowther e Don Woods, utilizando o arquivo de dados \texttt{advent.dat} \textcite{adventure_original_sources}. O pacote permite jogar em dois modos, no \emph{prompt} do Python e em terminal do sistema operacional. Além disso, disponibiliza \textit{walkthroughs} automatizados na pasta de testes.

\subsection{Descrição do jogo}

\textit{Colossal Cave Adventure}, também conhecido como \textit{ADVENT} ou simplesmente \textit{Adventure}, é amplamente reconhecido como o primeiro jogo de aventura baseado em texto da história, criado por Will Crowther em meados de 1975 e expandido por Don Woods em 1976. 

Ambientado em uma caverna repleta de tesouros, criaturas e labiríntos, o jogador interage por comandos de texto, como \textit{"GO NORTH"} ou \textit{"GET LAMP"}. O sistema responde com descrições que narram as consequências das ações.

Como observa \textcite{dibbell1998mytinylife}, o jogo automatiza o papel do mestre (\textit{Dungeon Master}) característico de campanhas de \textit{Dungeons and Dragons}. Suas descrições textuais simulam a fala do mestre (“\textit{YOU ARE IN A MAZE OF TWISTY LITTLE PASSAGES, ALL ALIKE}”).  

“Como qualquer programa significativo, \textit{Adventure} expressava a personalidade e o ambiente de seus autores.” \textcite{levy2010hackers}

Will Crowther e sua ex-esposa, Patricia Crowther, ambos programadores e espeleólogos, participaram do mapeamento do sistema de cavernas \textit{Mammoth Cave}. No verão de 1974, enquanto jogava campanhas de \textit{Dungeons and Dragons}, Will começou o desenvolvimento do seu jogo utilizando o Fortran. O mapa utilizado no jogo foi inspirado diretamente nos levantamentos realizados pelo casal durante as expedições à Mammoth Cave, construindo no código a estrutura real da caverna.

Como o próprio Will Crowther relata, a ideia do jogo surgiu da combinação entre suas experiências em espeleologia e seu interesse por \textit{Dungeons and Dragons}: “Eu estava envolvido em um jogo de interpretação de papéis... e tive uma ideia que combinasse o meu interesse por exploração de cavernas com algo que também fosse um jogo para as crianças...” \textcite{peterson1983genesis}.

\textcite{levy2010hackers} conta como inicia a colaboração de Donald Woods, um pesquisador da \textit{Stanford Artificial Intelligence Laboratory} (SAIL), em 1976. Após ter contato com uma prévia do jogo, Woods entrou em contato com Crowther, obteve sua permissão e passou a expandir o código. Sua versão incorporou novos puzzles, criaturas e elementos de fantasia inspirados na obra de Tolkien, além de um sistema de pontuação que estabelecia um objetivo ao jogador. A versão combinada de Crowther e Woods é um marco na história da interação humano-computador.

\subsection{}
Como o jogo não possui documentação original, utilizei o artigo de \textcite{jerz2007colossal} como referência para compreender a estrutura e o funcionamento do código. O autor recupera e examina o código-fonte escrito por Will Crowther, a partir de um backup preservado no SAIL. Jerz descreve as seis tabelas centrais que organizam os dados do jogo: descrições longas, rótulos curtos das salas, dados de mapa, vocabulário agrupado, estados estáticos e eventos ou dicas.  

Essa arquitetura de dados é mantida na reimplementação em Python, embora expandida para doze seções, resultado da integração da versão de Don Woods \textcite{rhodes_adventure_py}. A leitura e o processamento dessas tabelas ocorrem por meio do arquivo \texttt{advent.dat}, que preserva a semântica e a estrutura do código original. 

As seis tabelas descritas por Crowther estruturam o mundo do jogo e suas interações:
\begin{enumerate}
    \item \textbf{Long Descriptions}: textos descritivos longos que definem os ambientes e estados narrativos;
    \item \textbf{Short Room Labels}: nomes curtos usados internamente para identificar locais e facilitar a navegação;
    \item \textbf{Map Data}: conexões topológicas entre os ambientes e as direções de movimento possíveis;
    \item \textbf{Grouped Vocabulary Keywords}: agrupamento de palavras-chave e comandos interpretados pelo sistema;
    \item \textbf{Static Game States}: variáveis e condições fixas que controlam a lógica do jogo;
    \item \textbf{Hints and Events}: mensagens de ajuda, eventos dinâmicos e respostas a situações específicas.
\end{enumerate}

As outras seis adicionadas na versão em colaboração com Woods são:

\begin{enumerate}
  \item \emph{Object locations} — localização dos objetos;
  \item \emph{Action defaults} — mensagens padrão ligadas a verbos de ação;
  \item \emph{Liquid assets / flags} — \texttt{COND} por sala (luz, líquidos, restrições do pirata, bits de dicas);
  \item \emph{Class messages} — faixas de pontuação e mensagens de classificação do jogador;
  \item \emph{Hints} — dicas (turnos necessários, penalidade, pergunta e resposta);
  \item \emph{Magic messages} — mensagens de inicialização e manutenção.
\end{enumerate}

\paragraph{Tabela 1 – Long Descriptions.}  
A Tabela 1 contém descrições extensas dos ambientes do jogo. Com entradas identificadas de 1 a 140, ela define os textos apresentados ao jogador em diferentes locais. Cada linha representa uma sala ou estado narrativo. Parte dessas descrições refere-se diretamente a locais da caverna, como o trecho “\textit{YOU ARE STANDING AT THE END OF A ROAD BEFORE A SMALL BRICK BUILDING}”, enquanto outras descrevem situações de falha ou eventos inesperados, como “\textit{YOU ARE AT THE BOTTOM OF THE PIT WITH A BROKEN NECK}”.  

Exemplos:  
\begin{itemize}
    \item 1	\textit{AROUND YOU IS A FOREST.  A SMALL STREAM FLOWS OUT OF THE BUILDING AND DOWN A GULLY}.
    \item 2	\textit{YOU HAVE WALKED UP A HILL, STILL IN THE FOREST.  THE ROAD SLOPES BACK DOWN THE OTHER SIDE OF THE HILL.  THERE IS A BUILDING IN THE DISTANCE.}
    \item 3	\textit{YOU ARE INSIDE A BUILDING, A WELL HOUSE FOR A LARGE SPRING.}
\end{itemize}

\paragraph{Tabela 2 – Short Room Labels.}  
A Tabela 2 contém rótulos curtos correspondentes às localizações/ambientes do jogo. Com entradas numeradas de 1 a 130, nem todas as salas ou estados definidos em \textit{Long Descriptions} possuem equivalentes resumidos.  

Exemplos:  
\begin{itemize}
    \item 1 \textit{YOU'RE AT END OF ROAD AGAIN.}
    \item 3 \textit{YOU'RE INSIDE BUILDING.}
    \item 18 \textit{YOU'RE IN NUGGET OF GOLD ROOM.}
    \item 19 \textit{YOU'RE IN HALL OF MT KING.}
\end{itemize}

\paragraph{Tabela 3 – Map Data.}
A Tabela 3 codifica a topologia do mundo do jogo e as regras de navegação, funcionando como um grafo dirigido rotulado. A primeira coluna indica o ambiente em que o jogador se encontra, a segunda define o ambiente de destino, e as colunas subsequentes agrupam os vocabulários que podem ser utilizados para realizar a transição entre os dois pontos. O mapeamento dos vocabulários é definido na Tabela 4.  

Em alguns casos, o valor do destino representa uma condição especial, e não uma simples sala. Se o número de destino for maior que 500, o jogo exibe uma mensagem da Tabela 6 e o jogador permanece no mesmo local; Se estiver entre 300 e 500, o valor indica um salto especial para um trecho de código do jogo.
   
Exemplos:
\begin{itemize}
  \item 1  2  2  44  29: o jogador se desloca do ambiente 1 ao aombiente 2, se utilizados os comando 2, 44 ou 29. 
  \item 3  1  3  11  32  44: o jogador se desloca do ambiente 2 ao ambiente 1 se utilizados os comando 3, 11, 32 ou 44.
\end{itemize}

\paragraph{Tabela 4 – Grouped Vocabulary Keywords.}
No código original em Fortran, toda entrada de texto era truncada nos cinco primeiros caracteres, de modo que o comando \textit{“inventory”}, por exemplo, poderia ser digitado simplesmente como \textit{“inven”}. A reimplementação em Python de \textcite{rhodes_adventure_py} preserva essa lógica.  

Os dados da tabela 4 são divididos em 4 grupos: o primeiro com id's entre 1 e 100 para movimento no jogo; com ids entre 1000 e 2000, trata de objetos manipuláveis ou características de cenário; com ids entra 2000 e 3000 são verbos de ação, se entre 3000 e 4000 são para casos especiais.

\begin{itemize}
  \item 1–100: verbos de movimento, utilizados para navegação no espaço do jogo;  
  \item 1000–2000: objetos e elementos de cenário manipuláveis;  
  \item 2000–3000: verbos de ação (\textit{carry}, \textit{attack}, \textit{drop}, etc.);  
  \item 3000–4000: verbos de casos especiais, geralmente associados a eventos ou mensagens específicas definidas na Tabela 6.  
\end{itemize}

Além dos comandos clássicos de navegação por bússola, "\textit{EAST}"/"\textit{E}", "\textit{WEST}"/"\textit{W}", "\textit{NORTH}"/"\textit{N}", "\textit{SOUTH}"/"\textit{S}", parte dos veros de movimentos são nomes de locais da caverna como "\textit{BEDQU}" (truncamento de \textit{Bedquilt}), "\textit{HOUSE}", "\textit{GATE}" e "\textit{FORES}" (\textit{forest}).

Exemplos:  
\begin{itemize}
    \item 2 \textit{ROAD}
    \item 3 \textit{ENTER}
    \item 3 \textit{DOOR}
    \item 3 \textit{GATE}
    \item 4 \textit{UPSTR}
    \item 5 \textit{DOWNS}
    \item 6 \textit{FORES}
\end{itemize}

Palavras de mesmo sentido/sinônimos possuem mesmo id, como "\textit{ENTER}", "\textit{DOOR}" e "\textit{GATE}".

\paragraph{Tabela 5 – Static Game States.}  
A Tabela 5 armazena descrições curtas que representam estados do jogo, correspondendo às mudanças permanentes no ambiente. Cada linha contém um número e uma mensagem descritiva.  

Quando o identificador está entre 1 e 100, a linha define a mensagem de inventário associada a um objeto, exemplo: “\textit{SET OF KEYS}” se refere a "\textit{KEYS}". Quando o identificador é um múltiplo de 100, a mensagem descreve uma propriedado do objeto. 

Exemplos:  
\begin{itemize}
    \item 1	SET OF KEYS
    \item 000	THERE ARE SOME KEYS ON THE GROUND HERE.
    \item 2	BRASS LANTERN
    \item 000	THERE IS A SHINY BRASS LAMP NEARBY.
    \item 100	THERE IS A LAMP SHINING NEARBY.
    \item 3	*GRATE
    \item 000	THE GRATE IS LOCKED.
    \item 100	THE GRATE IS OPEN.
\end{itemize}

\paragraph{Tabela 6 – Hints and Events.}
A Tabela 6 reúne mensagens arbitrárias usadas como dicas e como descrições de eventos pontuais. Essas mensagens não estão relacionadas a um ambiente ou objeto específicos, elas são acionadas por outras estruturas do jogo, como as tabelas 3, 4, 8 e 11.

Exemplos:
\begin{enumerate}
    \item 3	AXE AT YOU WHICH MISSED, CURSED, AND RAN AWAY.
    \item 6	NONE OF THEM HIT YOU!
    \item 13 I DON'T UNDERSTAND THAT!
    \item 24 YOU ARE ALREADY CARRYING IT!
    \item 33 I DON'T KNOW HOW TO LOCK OR UNLOCK SUCH A THING.
\end{enumerate}

\paragraph{Tabela 7 – Object Locations.}
A Tabela 7 define onde cada objeto surge no mundo do jogo e se ele é móvel ou fixo. Cada linha possui o identificador do objeto, a sala inicial, e um campo opcional que indica imobilidade (-1) ou uma segunda sala quando o objeto existe simultaneamente em dois lugares 

\begin{itemize}
  \item Sala inicial = 0: o objeto não aparece no mundo no início e só será criado por algum evento ou ação do jogador.
  \item Terceiro campo = -1: o objeto está fixo naquela sala (não pode ser carregado).
  \item Terceiro campo = número de sala: o objeto está presente em duas salas ao mesmo tempo, objetos com duas localizações são tratados como imóveis.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 1 3: objeto 1 (1001 - KEY, KEYS) começam na sala 3 (INSIDE BUILDING).
  \item 2 3: objeto 2 (1002 - LAMP, HEADL, LANTE) começam na sala 3 (INSIDE BUILDING).
  \item 3 8 9: objeto 3 (1003 - grate) existe nas salas 8 e 9 simultaneamente (8 - YOU'RE OUTSIDE GRATE, 9 - YOU'RE BELOW THE GRATE.).
  \item (9 - DOOR) (94 - YOU ARE AT ONE END OF AN IMMENSE NORTH/SOUTH PASSAGE.)
  \item 9 94 -1: objeto 9 (1009 - DOOR) é fixo na sala 94 (94 - YOU ARE AT ONE END OF AN IMMENSE NORTH/SOUTH PASSAGE.).
  \item 15 0: objeto 15 (1015 - OYSTE) começa fora do mundo e aparece mais tarde.
\end{itemize}

\paragraph{Tabela 8 – Action Defaults.}  
A Tabela 8 define o comportamento padrão dos verbos de ação, associando cada identificador de verbo ao índice da mensagem correspondente na Tabela 6. Cada linha contém dois valores: o primeiro é o número do verbo de ação, e o segundo é o identificador da mensagem padrão que deve ser exibida.

Exemplos:  
\begin{itemize}
  \item 1 24: o verbo de ação associado ao id 1 (2001 - CARRY, TAKE, KEEP, CATCH, STEAL, CAPTU, GET, TOTE) e a mensagem 24 da tabela 6 (YOU ARE ALREADY CARRYING IT!). 
  \item 6 33: o verbo de ação associado ao id 6 (2006 - LOCK, CLOSE) e a mensagem 33 da tabela 6 (I DON'T KNOW HOW TO LOCK OR UNLOCK SUCH A THING.).
  \item 7 38: o verbo de ação associado ao id 7 (2007 - LIGHT, ON) e a mensagem 38 da tabela 6 (YOU HAVE NO SOURCE OF LIGHT.). 
\end{itemize}

\paragraph{Tabela 9 – Liquid Assets, Etc.}
A Tabela 9 define os bits de condição associados a cada sala, controlando luz, líquidos, presença de inimigos e zonas de interesse para as rotinas de dicas. Cada linha contém um identificador de bit e uma lista de até vinte localizações nas quais esse bit é ativado. O jogo usa esses bits para determinar o comportamento dinâmico de cada ambiente.

\begin{itemize}
  \item 0: indica que o ambiente está naturalmente iluminado.
  \item 1: tipo de líquido usado em conjunto com o bit 2. Quando o bit 2 está ativo, este bit diferencia óleo (1) de água (0).
  \item 2: marca as salas que contêm água ou óleo.
  \item 3: impede que o pirata apareça ali, exceto quando persegue o jogador.
  \item 4: jogador tentando entrar na caverna.
  \item 5: tentativa de capturar o pássaro.
  \item 6: interação com a cobra.
  \item 7: perdido no labirinto.
  \item 8: refletindo no quarto escuro.
  \item 9: na área final Witt's End.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 0 1 2 3 4 5 6 7 8 9 10 100 115 116 126: salas naturalmente iluminadas próximas à entrada.
  \item 2 1 3 4 7 38 95 113 24: presença de líquido (água ou óleo) nessas salas.
  \item 9 108: marca a área final do jogo, Witt’s End.
\end{itemize}

\paragraph{Tabela 10 – Class Messages.}
A Tabela 10 contém as mensagens de classificação do jogador de acordo com a pontuação total atingida ao final da partida. Cada linha associa um limite superior de pontuação a uma mensagem que descreve o título ou o nível de habilidade alcançado.  

Exemplos:
\begin{itemize}
  \item 35: YOU ARE OBVIOUSLY A RANK AMATEUR.  BETTER LUCK NEXT TIME.
  \item 100: YOUR SCORE QUALIFIES YOU AS A NOVICE CLASS ADVENTURER.
  \item 130: YOU HAVE ACHIEVED THE RATING: ‘EXPERIENCED ADVENTURER’.
  \item 200: YOU MAY NOW CONSIDER YOURSELF A ‘SEASONED ADVENTURER’.
  \item 250: YOU HAVE REACHED ‘JUNIOR MASTER’ STATUS.
  \item 300: MASTER ADVENTURER CLASSES C.
  \item 330: MASTER ADVENTURER CLASSES B.
  \item 349: MASTER ADVENTURER CLASSES A.
  \item 9999: ALL OF ADVENTUREDOM GIVES TRIBUTE TO YOU, ADVENTURER GRANDMASTER!
\end{itemize}

\paragraph{Tabela 11 – Hints.}
A Tabela 11 associa dicas contextuais a condições determinadas de jogo.  
Cada linha contém cinco valores:  

\begin{itemize}
  \item O primeiro valor vincula a dica a uma condição definidos na Tabela 9.
  \item O segundo valor define quantos turnos o jogador deve gastar no mesmo estado antes da dica ser oferecida.
  \item O terceiro valor representa a penalidade subtraída da pontuação total ao aceitar a ajuda.
  \item Os dois últimos valores apontam para mensagens da Tabela 6: a pergunta inicial e a resposta.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 4 4 2 62 63 — Bit 4 (entrada da caverna): após 4 turnos no local, o jogo exibe a pergunta 62 (Do you need help getting inside?) e, se aceita, mostra a resposta 63 (Perhaps you should explore the grate.), descontando 2 pontos.
  \item 6 8 2 20 21 — Bit 6 (cobra): depois de 8 turnos, o jogador recebe uma dica para resolver o enigma da serpente.
  \item 7 75 4 176 177 — Bit 7 (labirinto): após 75 turnos perdido, é oferecida uma dica de saída, com penalidade de 4 pontos.
  \item 8 25 5 178 179 — Bit 8 (quarto escuro): a dica surge depois de 25 turnos, custando 5 pontos.
\end{itemize}

\paragraph{Tabela 12 – Magic Messages.}
A Tabela 12 contém as chamadas \textit{Magic Messages}, um conjunto de mensagens reservadas utilizadas pelos modos de inicialização, manutenção e administração do jogo.  
Embora seu formato seja idêntico ao da Tabela 6, elas são separadas para facilitar o acesso e o controle das rotinas especiais do sistema. Cada linha contém um identificador e um texto associado.agens internas do sistema.

Exemplos
\begin{itemize}
  \item 1 \textit{A LARGE CLOUD OF GREEN SMOKE APPEARS IN FRONT OF YOU… HE MAKES A SINGLE PASS OVER YOU WITH HIS HANDS, AND EVERYTHING FADES AWAY INTO A GREY NOTHINGNESS.}
  \item 2 \textit{EVEN WIZARDS HAVE TO WAIT LONGER THAN THAT!}
  \item 3 \textit{I'M TERRIBLY SORRY, BUT COLOSSAL CAVE IS CLOSED. OUR HOURS ARE:}
  \item 4 \textit{ONLY WIZARDS ARE PERMITTED WITHIN THE CAVE RIGHT NOW.}
\end{itemize}

\newpage

\printbibliography

\end{document}
