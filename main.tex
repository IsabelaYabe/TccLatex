\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Idioma e tipografia
\usepackage[brazilian]{babel}
\usepackage{csquotes}
\usepackage{lmodern}
\usepackage{microtype}

% Layout e recursos básicos
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bookmark}

\emergencystretch=2em
\cleardoublepage
\pagenumbering{arabic}

% Matemática e teoremas (essencial só se você usa)
\usepackage{amsmath,amssymb,amsthm}

% Tabelas em paisagem e colunas flexíveis
% \usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{array} % para \newcolumntype
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
\usepackage{rotating} % para sidewaystable/sideways


% Bibliografia ABNT numerada
\usepackage[
  backend=biber,
  style=abnt,
  sorting=none,
  giveninits=true,
  uniquename=false,
  doi=false,
  isbn=false,
  url=false,
  language=brazil,
  scbib,
  ittitles,
  justify
]{biblatex}
\addbibresource{refs.bib} % ← caminho corrigido



% ======= PADRONIZAÇÃO PARA A TABELA MDRE =======

% Coluna flexível "Y" (se ainda não tiver)
% \usepackage{tabularx,booktabs,ragged2e,array}
% \newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}

% 1) Vocabulário controlado (sempre em SMALL CAPS):
% force medium series inside \textsc to avoid requesting a bold small-caps font (bx/sc)
\newcommand{\Static}{{\mdseries\textsc{Estático}}}
\newcommand{\Dynamic}{{\mdseries\textsc{Dinâmico}}}
\newcommand{\Hybrid}{{\mdseries\textsc{Híbrido}}}
\newcommand{\Comp}{{\mdseries\textsc{Compreensão}}}
\newcommand{\Redoc}{{\mdseries\textsc{Redocumentação}}}
\newcommand{\Mig}{{\mdseries\textsc{Migração}}}
\newcommand{\Quali}{{\mdseries\textsc{Qualidade}}}

% 2) Macros para setas e encadeamentos:
\newcommand{\ctoa}{\(\text{Código} \rightarrow \text{AST}\)}
\newcommand{\atoxi}{\(\text{AST} \rightarrow \text{IM}\)}   % IM = modelo intermediário
\newcommand{\imtoxml}{\(\text{IM} \rightarrow \text{XML}\)}
\newcommand{\imtomdl}{\(\text{IM} \rightarrow \text{UML}\)}
\newcommand{\tmtomdl}{\(\text{T2M/M2M} \rightarrow \text{UML}\)}
\newcommand{\xtoSeq}{\(\rightarrow \text{UML Sequência}\)}
\newcommand{\xtoClass}{\(\rightarrow \text{UML Classe}\)}
\newcommand{\xtoAct}{\(\rightarrow \text{UML Atividade}\)}

% 3) Abreviações de ferramentas (consistentes):
\newcommand{\EMF}{Eclipse/EMF}
\newcommand{\UMLtwo}{UML2}
\newcommand{\PlantUML}{PlantUML}
\newcommand{\JavaParser}{JavaParser}

% 4) Formato da célula “Aspecto”: Técnica ; Objetivo(s)
%    Ex.: \Static; \Comp/\Redoc (estrutura + comportamento)

% 5) Formato da célula “Técnica/Transformação”:
%    Use sempre cadeia com “→”, negrite elementos-chaves e padronize nomes.
%    Ex.: Código → AST → \textbf{IM(XML)} → T2M/M2M → \textbf{UML2}
%
% 6) Formato da célula “Validação”:
%    [tipo de evidência; dataset/projetos; métrica(s) ou avaliação; nota curta]
%    Ex.: OSS (9 projetos, 2640 classes); AUC=0.73; custo de rótulo 10%

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{0cm}
        
            \includegraphics[width=0.5\textwidth]{Images/Logo_FGV.png} 
            
        \vspace{1.5cm}
        \large
        
        Ciência de Dados e I.A.\\
        Escola de Matemática Aplicada\\
        Fundação Getúlio Vargas\\

        \vspace{1cm}  
    
        \Large
        Engenharia de Requisitos
            
        \vspace{2cm}
        
        \vspace{0.25cm}

        \Huge \textbf{Proposta de TCC} \\ 
        \vspace{0.5cm}
        \huge \textbf{LLM para Engenharia de Requisitos}
        \vspace{3.6cm}
        
        \large
                Aluno: Isabela Yabe\\
                Orientador: Rafael de Pinho André\\
                Escola de Matemática Aplicada, FGV/EMAp \\
                Rio de Janeiro - RJ.
        \vfill
            
        \vspace{0.8cm}  
        
        Rio de Janeiro, 2025
            
    \end{center}
\end{titlepage}
\newpage
\pagenumbering{roman}
\tableofcontents

\newpage
\pagenumbering{arabic}

\section{Resumo}

\section{Resumo}
 Este trabalho investiga se é possível recuperar artefatos de requisitos, em particular diagramas de casos de uso, diretamente a partir de sistemas implementados em Python, combinando análise estática de código e técnicas recentes de representação semântica com \textit{Large Language Models} (LLMs).

A proposta parte da \textit{Abstract Syntax Tree} (AST) do código-fonte, tratada como modelo intermediário em um fluxo de \textit{Model-Driven Reverse Engineering} (MDRE). Sobre essa estrutura, são extraídas \textit{features} estruturais (tipo de nó, escopo, relações de chamada) e textuais (nomes, docstrings, comentários), que alimentam um encoder de nós e uma GNN responsável por produzir embeddings semântico-estruturais do arquivo. A partir desses embeddings, métodos públicos são identificados como candidatos a casos de uso e agrupados por similaridade semântica, enquanto o grafo de chamadas fornece relações de dependência entre casos.

O resultado esperado é um processo de redocumentação capaz de gerar diagramas de casos de uso em \textit{PlantUML} a partir de código Python, preservando a semântica observada e oferecendo uma visão de alto nível do sistema. A principal contribuição é aproximar engenharia de requisitos e engenharia reversa ao mostrar como LLMs e modelos de código orientados por AST podem apoiar a recuperação de requisitos em cenários em que a documentação está ausente ou desatualizada.

\section{Introdução}

A engenharia de software estuda e avalia métodos capazes de aproximar o código-fonte da linguagem natural. Essa busca se manifesta em duas vertentes complementares: a interação com o usuário final e a comunicação entre os próprios desenvolvedores. 

Este estudo fundamenta-se em autores que defendem o desenvolvimento estruturado e orientado ao usuário, projetado a partir da visão e das necessidades de quem utiliza o sistema, e não apenas da estrutura interna ou das preferências de quem o desenvolve. Essa perspectiva deu origem a princípios de design centrados na função e no comportamento observável do sistema, enfatizando que a organização do código deve refletir a experiência do usuário e os fluxos de interação previstos. 

\textcite{yourdon1979structured} descrevem o processo tradicional de desenvolvimento de software como uma cadeia de tradução sucessiva: o diálogo entre o proprietário do produto, o usuário e o analista é continuamente reinterpretado pelo engenheiro de requisitos, pelo designer e pelo programador, conforme ilustrado na Figura~\ref{fig:cadeia_traducao_constantine1979}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{Images/diagrams/fluxo_info.png}
  \caption{cadeia de tradução de requisitos segundo Constantine \(1979\).}
  \label{fig:cadeia_traducao_constantine1979}
\end{figure}

Cada etapa dessa cadeia implica a perda ou distorção de parte do significado original do usuário, o que pode resultar em comportamentos apenas próximos ao desejado. Diante disso, os autores propõem o projeto estruturado, cujo ponto inicial é a clareza e a visibilidade das decisões e atividades envolvidas, promovendo uma compreensão compartilhada e garantindo que o design reflita as intenções originais do sistema.

\subsection{Problematização}

Com o mesmo intuito de tornar o comportamento do sistema visível e compreensível, surge a modelagem de casos de uso como um instrumento de unificação entre requisitos, design e usabilidade. Segundo \textcite{booch1999unified}, nenhum sistema existe isoladamente: todo sistema relevante interage com atores, humanos ou automáticos, que esperam comportamentos previsíveis. O diagrama de casos de uso permite que analistas e desenvolvedores discutam o comportamento do sistema sem se prender aos detalhes da implementação, oferecendo uma linguagem comum e verificável para representar comportamentos.

Autores posteriores ampliaram essa discussão para o nível do código, enfatizando a necessidade de que o código não seja apenas executável, mas também compreensível. Como sintetiza \textcite{fowler2018refactoring}, “qualquer tolo escreve um código que um computador possa entender; bons programadores escrevem código que seres humanos possam entender”.

Entretanto, a legibilidade do código, por si só, não substitui a documentação de requisitos. Enquanto o código explica como o sistema se comporta, a documentação torna explícito por que ele deve se comportar assim. Segundo \textcite{sommerville1997requirements}, a documentação de requisitos atua como um contrato conceitual entre usuários, analistas e desenvolvedores, garantindo o alinhamento entre o comportamento implementado e as expectativas de negócio. Quando essa documentação falta ou envelhece, a legibilidade do código torna-se o principal ponto de apoio para reconstruir as intenções originais, o que representa um desafio na manutenção e evolução de sistemas legados.

\subsection{Questão e hipótese}

Se o código é um texto escrito para ser lido por humanos, então suas palavras, nomes e estruturas carregam pistas úteis sobre o que o sistema faz e para quem. Partindo dessa premissa, pergunta-se: é possível reconstruir casos de uso a partir do código-fonte, combinando análise estrutural e interpretação semântica automatizada?

A hipótese deste trabalho é que técnicas de representação semântica, como embeddings e \textit{Large Language Models} (LLMs), quando aplicadas sobre estruturas abstratas do código, como a \textit{Abstract Syntax Tree} (AST), podem viabilizar a reconstrução de artefatos de alto nível, como diagramas de casos de uso, mesmo na ausência de documentação formal.

\subsection{Objetivos}

O objetivo geral deste trabalho é propor um processo de redocumentação automatizada capaz de gerar diagramas de casos de uso a partir do código-fonte, preservando a semântica do sistema original.
Para isso, o método combina:

\textcite{Bruneliere2010MoDisco} o MoDisco, um framework genérico para engenharia reversa orientada por modelos (Model-Driven Reverse Engineering — MDRE). Ele sugere resumirmos os sistemas em modelos, uma estrutura mais homogênea. A principal ideia é recuperar modelos existentes no sistema. O processo é dividido em duas fases, descoberta do modelo e compreensão do modelo. Na fase de descoberta, um componente chamado discoverer extrai informações do código-fonte, dados brutos, documentações e artefatos disponíveis. Passando estas informações para uma representação da estrutura do sistema. Já na fase de compreensão, o conteúdo desse modelo é analisado e transformado em representações de alto nível, diagramas, métricas ou relatórios, que podem servir à redocumentação, à modernização de sistemas ou à análise de qualidade.

A partir dessa arquitetura, adotaremos a mesma lógica de abstração proposta por \textcite{tonella2007reverse}, utilizando uma representação sintática reduzida do código-fonte que preserva apenas os elementos essenciais ao fluxo de objetos, criações, atribuições e chamadas, e ignora instruções de controle. Essa simplificação torna possível construir a Abstract Syntax Tree (AST) como modelo intermediário, permitindo representar a estrutura e os diagramas de casos de uso.

Esse tipo de investigação é definido por \textcite{chikofsky1990reverse} como \textit{Redocumentation} em \textit{Reverse engineering}, ou seja, engenharia reversa com foco em redocumentação, no sentido de criar representações de abstração do sistema existente, destinadas à leitura humana, sem alterar o comportamento do software. 

Além da linguagem abstrata, este trabalho incorpora informações semânticas extraídas diretamente das \textit{docstrings}, comentários e nomenclaturas do código. Esses elementos textuais são tratados como extensões dos objetos, pois também comunicam intenções, objetivos e relações entre entidades. Com o apoio de \textit{Large Language Models} (LLMs), essas evidências são analisadas de forma contextual, permitindo inferir papéis, objetivos e interações que não estão explicitamente representados nas chamadas ou estruturas do código.

Dessa forma, o processo de redocumentação combina a análise estrutural, que descreve como os objetos estão correlacionados, e a análise semântica, que interpreta o vocabulário interno do sistema revelando as intenções dos desenvolvedores. 

\subsection{Relevância}

Este trabalho contribui para auxiliar desenvolvedores durante a codificação e também na compreensão de sistemas sem documentação. Ao gerar visões de alto nível do sistema, especificamente casos de uso, a proposta facilita a compreensão e as interações entre componentes. 

Segundo \textcite{larman2002applying}, os casos de uso não apenas documentam funcionalidades, mas representam um instrumento de convergência entre analistas, projetistas e programadorea. Em contextos dinâmicos, casos de uso bem definidos apoiam a priorização de requisitos, a validação de comportamentos e a manutenção de uma visão compartilhada do sistema, mesmo diante de mudanças constantes.

Embora a maioria dos estudos sobre Model-Driven Reverse Engineering (MDRE) e redocumentação concentre-se em linguagens como Java, este trabalho propõe uma abordagem direcionada à linguagem Python, que, segundo o TIOBE Index (2025), mantém-se como a linguagem mais popular globalmente. 

Por fim, além de oferecer uma nova aplicação prática de Large Language Models na engenharia de software, o estudo propõe uma ponte entre engenharia de requisitos e engenharia reversa, reforçando a ideia de que compreender um sistema começa por compreender seu código, não apenas como sequência de instruções, mas como expressão das intenções humanas que lhe deram origem.

\section{Revisão literária}

A revisão tem o objetivo de compreender o estado da arte das abordagens de engenharia reversa que partem de código-fonte e produzem artefatos de alto nível, como diagramas UML. Para garantir uma análise sistemática e comparável entre diferentes propostas, foram definidas perguntas de pesquisa (\textit{Research Questions — RQs}) que orientam a coleta e síntese dos dados extraídos dos estudos selecionados.

\begin{itemize}
  \item \textbf{RQ1.} Em quais linguagens e domínios as abordagens que partem de código-fonte foram aplicadas?
  \item \textbf{RQ2.} Quais modelos/artefatos de alto nível são gerados?
  \item \textbf{RQ3.} Qual aspecto é privilegiado (estático, dinâmico, híbrido) e com qual objetivo (compreensão, redocumentação, migração, qualidade)?
  \item \textbf{RQ4.} Quais técnicas e transformações viabilizam a passagem do código para o modelo de alto nível?
  \item \textbf{RQ5.} Quais ferramentas/frameworks são utilizados?
  \item \textbf{RQ6.} Como as abordagens são validadas e com que qualidade prática?
\end{itemize}

A coleta dos estudos seguiu uma estratégia sistemática de busca em bases reconhecidas, IEEE Xplore e ACM Digital Library no período de 2015 a 2025.

A query se estrutura na combinação de três blocos temáticos:

\begin{itemize}
  \item ("Abstract": "MDRE" OR "reverse engineering" OR "model driven reverse engineering" OR "design recovery")
  \item ("Abstract": "UML" OR "UML class diagram" OR "UML activity diagram" OR "UML sequence diagram" OR "UML models" OR "Diagram")
  \item: ("Abstract": "static analysis" OR "source code analysis" OR "abstract syntax tree" OR "AST" OR "text-to-model" OR "T2M" OR "parser" OR "source code" OR "parsing")
\end{itemize}

Foram incluídos apenas os estudos que propõem uma abordagem de engenharia reversa aplicada à geração de modelos UML (classes, atividades ou sequência) diretamente a partir do código-fonte.
  
Foram excluídos os trabalhos que se enquadravam em uma ou mais das seguintes categorias:
\begin{itemize}
  \item Foco em forward engineering ou geração de código.
  \item Estudos centrados em rastreabilidade ou anti-padrões.
  \item Trabalhos puramente empíricos ou teóricos sem proposta de transformação automatizada.
  \item Abordagens puramente dinâmicas.
\end{itemize}

Com base nos critérios de inclusão e exclusão, foram selecionados os seguintes estudos para análise detalhada:

\begin{itemize}
  \item A Model Driven Reverse Engineering Framework for Generating High Level UML Models From Java Source Code (2019).
  \item Condensing Class Diagrams With Minimal Manual Labeling Cost (2016). (parte do diagrama e aperfeiçoa)
  \item Enhancing Model-Driven Reverse Engineering Using Machine Learning (2024).
  \item Reverse Engineering of Source Code to Sequence Diagram Using Abstract Syntax Tree (2016).
  \item Towards a New Hybrid Approach of the Reverse Engineering of UML Sequence Diagram (2016).
  \item WIP: Generating Sequence Diagrams for Modern Fortran (2017).
\end{itemize}

\begin{sidewaystable}[p]
\centering
\scriptsize % Mudar o tamanho da fonte para caber melhor
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textheight}{Y Y Y Y Y Y Y}
\toprule
\textbf{Autores / Referência} &
\textbf{Linguagem / Domínio} & % Verde
\textbf{Modelo Gerado} & % Azul
\textbf{Aspecto} & % Laranja
\textbf{Técnica / Tipo de Transformação} & % Roxo
\textbf{Ferramenta / Framework} & % Cinza
\textbf{Validação / Estudo de Caso} \\ % Rosa
\midrule


\textcite{zhang2016j2x} &
Java; pequenos sistemas OO (eLib, Minesweeper, Blog, PayrollSys, myAlgsLib) &
UML Classe; UML Sequência &
Estático — compreensão, manutenção/redocumentação &
Código→AST→J2X; mapeamentos (gen./impl./assoc./dep.); sentenças simplif.→OFG; CFG+OFG→Sequência &
J2UML; JavaCC; Dom4j; J2X (DTD/XML) &
5 casos pequenos; acurácia: classes 96,4–100\%; relações 65,0–90,4\% \\

\textcite{yang2016condensing} &
Java; sistemas OO &
UML Classe (condensado) &
\Static; \Comp/\Redoc; estrutural &
\textbf{Extração de métricas (SDMetrics)} $\rightarrow$ Normalização (z-score) $\rightarrow$ \textbf{k-means clustering} $\rightarrow$ Random under-sampling $\rightarrow$ \textbf{Ensemble learning (Random Forest)} $\rightarrow$ diagrama condensado &
MagicDraw; SDMetrics; Random Forest; Windows 7 &
OSS (9 projetos, 2640 classes); AUC=0.73; custo de rótulo=10\%; teste de Wilcoxon e Cliff’s $\delta$ \\

\textcite{Fauzi2016AST} &
Java; sistemas orientados a objetos &
UML Sequência (comportamental) &
\Static; \Comp/\Redoc &
\textbf{Código} $\rightarrow$ \textbf{AST (JavaParser)} $\rightarrow$ \textbf{DFS pós-ordem} $\rightarrow$ \textbf{PlantUML (Seq)} &
REVUML; \JavaParser; \PlantUML &
126 casos de teste (8 categorias; geração correta e consistente de diagramas \\ 


\textcite{baidada2016hybrid} &
Java/Genérico; aplicações OO &
UML Sequência (HLSD) &
\textbf{Híbrido} (Estático + Dinâmico); Compreensão/Redocumentação (comportamento) &
CFG$\!\rightarrow$entradas; execuções+traços (filtro); traços$\!\rightarrow$CPN; CPN$\!\rightarrow$UML SD &
Sem ferramenta nominal; UML 2; instrumentação/VM/debugger; CPN (IR) &
Sem validação; futuro \\

\textcite{leatongkam2017generating} &
Fortran OO; computação científica e engenharia &
UML Classe; UML Sequência; modelo intermediário XMI &
Estático — compreensão e redocumentação &
Regras de mapeamento código $\rightarrow$ UML (OMG); parsing estático; árvore sintática; geração XMI $\rightarrow$ importação ArgoUML &
ForUML (extensão); ArgoUML; padrão OMG UML/XMI;&
\textit{Work in progress} \\

\textcite{Sabir2019MDRE} &
Java (sistemas legados orientados a objetos) &
UML \emph{Class Diagram} + \emph{Activity Diagram} (em pacote UML) &
Estático; objetivo: compreensão/redocumentação &
T2M/M2M em duas fases: Parser$\rightarrow$AST$\rightarrow$IM(XML) e mapeamentos IM$\rightarrow$UML (classe/atividade) &
Eclipse + UML2/EMF; JavaParser (IMD); (Papyrus/StarUML/Rational Rose na validação manual) &
Comparação especialista (modelos manuais vs. gerados), 5 estudos de caso; ATM e Amadeus descritos \\ 


\textcite{siala2024enhancing} &
Java; Python; sistemas legados &
UML Classe; OCL &
Estático — compreensão, redocumentação e migração &
código $\rightarrow$ tokenização/simplificação $\rightarrow$ geração textual UML/OCL intermediária(LLM) $\rightarrow$ model repair $\rightarrow$ diagramas UML/OCL &
Graphviz; PlantUML; Modelio; AgileUML; LLM &
Comparação MDRE: dois estudos de caso; correção semântica, completude e compreensibilidade \\

\bottomrule
\end{tabularx}
\caption{Síntese comparativa dos estudos selecionados.}
\label{tab:sintese_comparativa}
\end{sidewaystable}

\subsection{Análise sistemática da literatura}

A partir da síntese da Tabela~\ref{tab:sintese_comparativa}, organizamos os achados por eixo (RQ1–RQ6).

\paragraph{RQ1 — Linguagens e domínios.}
Predomina o ecossistema \textbf{Java} em sistemas orientados a objetos, tanto em estudos estruturais quanto comportamentais (\textcite{zhang2016j2x, yang2016condensing, Fauzi2016AST, Sabir2019MDRE}). Há ampliação pontual para \textbf{Fortran OO} em contexto de computação científica (\textcite{leatongkam2017generating}) e menção tanto a \textbf{Java} quanto a \textbf{Python} em proposta recente com LLMs (\textcite{siala2024enhancing}). Em síntese, o corpus avaliado é fortemente dominado por Java, Python surge como alvo ainda subexplorado.

\paragraph{RQ2 — Modelos/artefatos gerados.}
A produção concentra-se em \textbf{UML Classe} e \textbf{UML Sequência}. Em \textcite{zhang2016j2x}, ambos são gerados a partir de um pipeline \textit{código} $\rightarrow$ \textit{J2X} $\rightarrow$ \textit{OFG/CFG} $\rightarrow$ \textit{UML} (Classe+Sequência) . \textcite{leatongkam2017generating} propõe estender o \emph{ForUML} para também extrair \textbf{Sequência} a partir de Fortran OO, exportando um \textbf{XMI} intermediário para visualização (e.g., ArgoUML) . \textcite{Fauzi2016AST} derivam \textbf{Sequência} diretamente da \textbf{AST}, com saída em \emph{PlantUML} (Seq), abordagem estática focada em interações. \textcite{Sabir2019MDRE} incluem \textbf{Activity} além de \textbf{Class}, gerando “modelos UML de alto nível” (classe + atividade) a partir de um modelo intermediário (UML2) . \textcite{yang2016condensing} não “geram” um novo tipo de diagrama, mas \emph{condensam} \textbf{diagramas de classe} via métricas + \emph{ensemble learning}, reduzindo a complexidade visual ao destacar “classes importantes” (AUC, testes de Wilcoxon/Cliff’s $\delta$) . Em contraste, \textbf{Casos de Uso} aparecem sobretudo como enquadramento conceitual para Sequência, mas não como artefato recuperado dos códigos analisados, sinalizando uma lacuna na redocumentação de requisitos a partir de código.

\paragraph{RQ3 — Qual aspecto é privilegiado (estático, dinâmico, híbrido) e com qual objetivo?}
No conjunto analisado, prevalece de forma nítida o aspecto \textbf{\Static}, quase sempre orientado à \textbf{\Comp/\Redoc}. Os trabalhos clássicos da vertente estrutural e comportamental, como \textcite{zhang2016j2x} e \textcite{Fauzi2016AST}, operam integralmente sobre o código (sem execução), partindo de \emph{parsing}/AST e passando por representações intermediárias (p.\,ex., J2X) ou travessias específicas (DFS pós-ordem) para derivar, respectivamente, diagramas de Classe e Sequência. Em \textcite{leatongkam2017generating}, a mesma orientação estática se mantém ao estender o ForUML para Fortran OO via regras de mapeamento e exportação XMI; e em \textcite{Sabir2019MDRE}, trata-se de um processo estático T2M com fluxo código$\rightarrow$AST$\rightarrow$IM$\rightarrow$UML, com objetivo voltado à compreensão e redocumentação. Ainda sob a ótica estática, \textcite{yang2016condensing} não cria um novo artefato, mas trata o \emph{pós-processamento} do diagrama de classes via métricas e \emph{machine learning}, reduzindo a complexidade visual sem recorrer a dados de execução. Em contraste com esse predomínio, \textcite{baidada2016hybrid} introduzem um caminho \textbf{\Hybrid} (estático + dinâmico) para Sequência: gera-se um conjunto de entradas a partir do CFG, coletam-se traços por instrumentação/VM, sintetiza-se uma IR (\textit{Intermeadiate Representation}) comportamental em \emph{Colored Petri Nets} e, então, mapeia-se para UML. Por fim, \textcite{siala2024enhancing} preserva o caráter \textbf{\Static} ao integrar LLMs como camada semântica (texto intermediário UML/OCL seguido de \emph{model repair}), ampliando o objetivo para \textbf{migração} em sistemas legados e apontando uma inflexão do “estrutural puro” para um \emph{estrutural + semântico}. 

\paragraph{RQ4 — Técnicas e transformações.}
As abordagens convergem em um esqueleto MDRE que encadeia \emph{Text-to-Model} e \emph{Model-to-Model}: análise sintática do código (\emph{parsing}) para \textbf{AST} ou \textbf{IR} textual, seguida de mapeamentos para o metamodelo UML. Ainda assim, diferem nos \emph{intermediários}, nos operadores de fluxo usados para recuperar comportamento e no quanto incorporam \emph{semântica} além da sintaxe. Em \textcite{zhang2016j2x}, o núcleo é a \textbf{J2X} (DTD/XML), uma IR que padroniza elementos de linguagem; o diagrama de classes surge de metadados extraídos dessa IR, enquanto o diagrama de sequência resulta da \textbf{integração OFG+CFG} (rastros de objetos + fluxo de controle) para identificar \emph{lifelines}, \emph{messages} e \emph{combined fragments} (\textsf{alt}/\textsf{opt}/\textsf{loop}) de modo inteiramente estático. \textcite{Fauzi2016AST} elimina o XML e vai direto da \textbf{AST} (JavaParser) para \emph{Sequence}, guiado por uma travessia \textbf{DFS pós-ordem} com registro de variáveis, resolução de herança/polimorfismo e marcação de estruturas condicionais/iterativas; a apresentação é automatizada via \textbf{PlantUML}. Já \textcite{Sabir2019MDRE} formalizam o pipeline clássico \textbf{T2M/M2M} em duas fases: da AST para um \textbf{Intermediate Model (XML/EMF)} e, então, do IM (\textit{Intermeadiate Model}) para \textbf{UML} (Classe + \emph{Activity} por operação), com regras de transformação implementadas no ecossistema Eclipse/UML2. O \textbf{híbrido} de \textcite{baidada2016hybrid} desloca a recuperação comportamental para uma IR executável: um \textbf{CFG} orienta a geração de entradas; execuções instrumentadas produzem \emph{traces} filtrados; esses traços são sintetizados como \textbf{Colored Petri Nets (CPN)} e finalmente mapeados para \emph{UML Sequence}, capturando paralelismo e operadores combinados. Em domínio não-Java, \textcite{leatongkam2017generating} mantém a análise estática por \textbf{regras formais de mapeamento} (Fortran OO $\rightarrow$ UML), uma \emph{tree node structure} análoga à AST, e geração de \textbf{XMI} para importação/visualização no ArgoUML, expandindo o ForUML para \emph{Sequence}. Duas linhas recentes aplicam \emph{aprendizado}: \textcite{yang2016condensing} não cria um novo artefato, mas \textbf{condensa} o diagrama de classes com um pipeline \emph{métricas $\rightarrow$ normalização (z-score) $\rightarrow$ k-means $\rightarrow$ under-sampling $\rightarrow$ \emph{ensemble} (Random Forest)}, priorizando classes “importantes” e reduzindo a complexidade visual; e \textcite{siala2024enhancing} introduz \textbf{LLMs} como camada \emph{semântica}: o código é tokenizado/simplificado, traduzido para uma \textbf{representação textual intermediária UML/OCL}, submetida a \emph{model repair} e convertida em diagramas (PlantUML/Graphviz/Modelio). 

\paragraph{RQ5 — Ferramentas/frameworks.}
O ecossistema técnico das abordagens analisadas agrega \emph{parsers}/geradores UML, frameworks MDE e utilitários de visualização/mineração. Em \textcite{zhang2016j2x}, a cadeia J2X apoia-se na \textbf{J2UML} (orquestração), no \textbf{JavaCC} (geração do parser/AST), em \textbf{DOM4J} (manipulação XML) e no próprio \textbf{J2X (DTD/XML)} como IR; os experimentos reportam ambiente Windows 32-bit (3 GB RAM; Core 2 Duo). Em \textcite{yang2016condensing}, para “condensar” diagramas de classes, utilizam-se \textbf{MagicDraw} (recuperação de Classe), \textbf{SDMetrics} (métricas), e \textbf{Random Forest} (classificador), com relatos do ambiente \textit{Windows 7} (64-bit). A \textcite{Fauzi2016AST} (REVUML) integra \textbf{JavaParser} (AST) e \textbf{PlantUML} (renderização do Sequência), dispensando IR XML intermediária. Em \textcite{baidada2016hybrid}, não há ferramenta nominal de ponta a ponta: a coleta se dá por \textbf{instrumentação}/\textbf{JVM}/\textbf{debugger}, a IR comportamental usa \textbf{Colored Petri Nets} (sugerindo uso de \textit{CPN Tools}), e o mapeamento segue \textbf{UML 2}. Em \textcite{leatongkam2017generating}, a extensão da \textbf{ForUML} gera \textbf{XMI (OMG)} para importação no \textbf{ArgoUML} (visualização de Sequência). No framework \textcite{Sabir2019MDRE} (Src2MoF), o gerador baseia-se em \textbf{Eclipse}+\textbf{UML2/EMF} e o \textbf{JavaParser} integra o IMD; ferramentas como \textbf{Papyrus}/\textbf{StarUML}/\textbf{Rational Rose} são citadas apenas para comparação manual, não no pipeline automático. Por fim, \textcite{siala2024enhancing} combina \textbf{AgileUML}/\textbf{OMG MDA} com \textbf{LLMs} (camada semântica) e usa \textbf{Graphviz}, \textbf{PlantUML} e \textbf{Modelio} para materializar \textit{UML/OCL} (fase M2V).

\paragraph{RQ6 — Validação e qualidade prática.}
A avaliação varia de \textbf{estudos de caso pequenos com acurácia estrutural} (classes 96{,}4–100\%, relações 65{,}0–90{,}4) \parencite{zhang2016j2x}, a \textbf{testes sistemáticos} de geração de sequência \parencite{Fauzi2016AST}, e \textbf{comparação especialista} sem métricas quantitativas \parencite{Sabir2019MDRE}. \textcite{yang2016condensing} recorre a \textbf{AUC} e testes estatísticos (Wilcoxon, Cliff’s $\delta$) para condensação de classes. \textcite{baidada2016hybrid} não reporta validação empírica. Em suma, há carência de avaliações \emph{comparativas} com ground truth e métricas padronizadas na maioria dos trabalhos.

\section{Implicações para este trabalho.}

Plano estático (objetivo: casos de uso a partir de código Python)

Código → AST (IM/IR).
Parser de Python para construir a AST como modelo intermediário.

Candidatos a casos de uso.
Heurística estática: funções/métodos “públicos” viram use cases candidatos.

Clusterização para agrupar intenções.
Embeddings de código e de AST (NFASRPR-TRANS) para representar o código fonte; K-means/HDBSCAN agrupam métodos por intenção/semântica.
– O centróide do cluster nomeia o caso de uso; demais métodos viram passos/serviços/extends/includes.

Relacionamentos estáticos.

include: subrotinas/serviços invocados por $\ge k$ casos de uso candidatos (reuso sistemático no grafo de chamadas).

extend: chamadas opcionais guardadas por condições (flags/feature toggles/validações) detectadas no AST/CFG; marcam “pontos de extensão” do caso de uso base.

generalização (atores e casos): herança/implementação e sobrecarga de nomes (métodos homônimos (Shape, TextShape, GeometryShape) em super/subclasses) indicam especializações; atores inferidos de camadas de borda (controllers, adapters, decorators de rota) - rever.

dependência entre casos: mapeada de chamadas entre métodos “públicos” → dependência entre use cases.

Geração (PlantUML).
Serialização dos nós (casos, atores) e arestas (include/extend/generalização/dependência) em PlantUML.

Extração de casos de uso a partir de métodos públicos. \textcite{pereira2011recovering}

Generalização por herança/nomes homônimos.
O mesmo trabalho mapeia herança estática para generalização.
Fonte usada: regras de transformação (ATL) — adotamos a mesma leitura para Python (classes e MRO).

Dependências entre casos a partir de chamadas.
Fonte usada: regra Dependency2Dependency — base para nossas arestas de dependência/include.

Embeddings estáticos guiados por AST.
\textcite{LiangHuang2024ASTLLM} descrevem um modelo com dois encoders (um para semântica lexical e outro para estrutura de AST) para sumarização de código.
Assim aproveitamos os embeddings para melhorar a qualidade dos clusters.

Implicações para este trabalho. Propomos redocumentação semântica a partir de Python, combinando (i) AST como modelo intermediário (IM/IR) e (ii) embeddings sensíveis à estrutura (NFASRPR-TRANS) para inferir intenção e papel de métodos (docstrings, comentários e nomes). Casos de uso são inferidos por métodos de borda; generalizações vêm de herança/assinaturas homônimas; include decorre de subrotinas reutilizadas; extend de ramos opcionais. A saída é um Use Case Diagram em PlantUML. (Base: Pereira — extração estática por métodos públicos; embeddings orientados a AST.)

código → AST → embeddings+clusterização → regras estáticas (include/extend/atores) → PlantUML

De onde vem: mapeamento “método público → caso de uso” e “chamada entre públicos → dependência” (Pereira), robustez semântica dos vetores via AST-encoder.

Representação: nós = {atores, casos}; arestas = {include, extend, generalização, dependência}.

Sinais estáticos:

Atores: classes/módulos de interface (controllers/views/adapters; decorators de rota/CLI).

include: função/método chamado por $\ge k$ casos distintos; baixa complexidade ciclomática; sem guardas específicas.

extend: chamadas condicionais (predicados sobre entrada/estado/feature-flag) marcando “pontos de extensão”.

generalização: herança + homonímia de assinatura nos métodos.

$score = \alpha \cdot (\text{freq. de reuso}) 
       + \beta \cdot (\text{centralidade no call graph})
       - \gamma \cdot (\text{força da guarda condicional})$

Saída: .puml gerado a partir do grafo.

Elementos e regras de extração (todos estáticos):

Casos de uso (UC): cada método público vira um UC com o mesmo nome do método (no seu caso, prefixe com o nome da classe para evitar colisões). Trecho-base (“Recovering Use Case Diagrams from Object Oriented Code”): “Basic use cases are extracted analyzing public methods. Each public method of a class corresponds to a use case whose name is the same as the method name.”

Generalização (entre UCs): quando houver herança e mesmo nome/assinatura entre métodos em classes distintas, gere generalização entre os UCs correspondentes: “For each pair of same-name methods which are members of different classes with a generalization between them, a generalization between use cases… is generated.”

Dependência entre UCs (aresta genérica): para cada chamada p.g() dentro de um método público m, crie dependência entre UC(m) e UC(g): “For each public method of a class, the method calls are analyzed to extract dependence relationships among basic use cases. Each message sent to an object corresponds to a dependence between use cases.”

$\longrightarrow$ Decisão do seu trabalho (100\% estático): manter apenas “Dependency” (sem classificar em <<include>>/<<extend>>), pois a própria autora afirma que o tipo (include/extend) “may be inferred by a dynamic analysis” — que você não quer usar.

Atores: não inferir automaticamente (manter passo manual/regras externas), pois “actors can not be automatically inferred”.

Passos do processo (estático): (1) gerar código abstrato/AST; (2) executar algoritmo de recuperação estática; (3) (dinâmico para classificar dependências) — omitido no seu trabalho; (4) definir UCs de alto nível.

Elevação de nível (agrupar UCs): aplicar heurísticas de clusterização para formar UCs mais abstratos: “use cases may be clustered in single more abstract use cases by applying simple heuristics”. (Heurísticas a/b do passo 4).

Observação prática: você pode manter todas as arestas como Dependency e, apenas na visualização, sinalizar candidatos a include (reuso alto/fan-in elevado) e candidatos a extend (dependências sob guarda condicional detectável no AST/CFG). Isso segue a letra do artigo (classificação formal requer dinâmica), mas entrega valor prático na redocumentação.

\section{Implementação}
2302
emânticos para apoio a tarefas de análise de código. A solução foi organizada em três camadas principais: (i) um núcleo de análise estática que constrói a AST e produz a estrutura intermediária TNode; (ii) uma arquitetura de microkernel baseada em passes plugáveis, responsável por enriquecer cada nó com metadados estruturais e semânticos; e (iii) uma camada de representação vetorial, que combina features estruturais e textuais em um encoder de nós e, em seguida, aplica uma GNN sobre o grafo da AST para obter embeddings por nó e um embedding global do arquivo.

O TNode é a representação intermediária que agrega a AST bruta com as informações derivadas pelos passes (Figura~\ref{fig:plugins}). Sua definição é organizada em blocos de campos, cada um associado a um grupo de passes especializado: Informações de caminho (\texttt{path\_info}); Informações de nomes e visibilidade (\texttt{names\_visibility} e \texttt{naming}); Tipo de método e assinatura de I/O (\texttt{method\_kind} e \texttt{io\_signature}); Tipo de classe (\texttt{class\_kind}); Documentação e comentários (\texttt{docs\_comments}).

Nenhum desses campos é oferecido diretamente pela AST nativa do Python; todos são inferidos pelo conjunto de passes executados.

Cada nó \texttt{TNode} recebe, além do nome simples (\texttt{name}), um nome qualificado (\texttt{qname}) construído a partir da pilha de classes e funções ativas durante a travessia da AST. O \texttt{qname} identifica de forma única o elemento no arquivo, por exemplo, \texttt{"Unit.accept"} para um método público da classe \texttt{Unit}. Esse identificador é utilizado posteriormente para mapear métodos a casos de uso e para construir relações de dependência entre casos de uso.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{drawio/Images/AST_Model_Plugins-plugins_order_package.drawio.png}
  \caption{Plugins.}
  \label{fig:plugins}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{drawio/Images/AST_Model_Plugins-tnode_seq.drawio.png}
  \caption{Diagrama de sequencia da construção do TNode.}
  \label{fig:diagrama_de_sequencia_da_construcao_do_tnode}
\end{figure}

Módulos principais da implementação do ASTCore (Figura~\ref{fig:arquitetura_astcore}):
\begin{itemize}
  \item \textbf{\texttt{astcore.model}}: define as classes centrais \texttt{Ctx} e \texttt{TNode}, utilizando \texttt{@dataclass} da biblioteca padrão do Python. Essa camada encapsula a representação intermediária e é independente dos passes de análise.
  \item \textbf{\texttt{astcore.walker}}: implementa as estratégias de travessia da AST nativa do Python (\texttt{recursive\_pre}, \texttt{iterative\_pre}, \texttt{bfs}), produzindo uma sequência de \texttt{TNode}s a partir de um \texttt{ast.Module}. Essa camada atua como um \emph{visitor} especializado, mas desacoplado dos detalhes de cada análise.
  \item \textbf{\texttt{pass\_plugins}}: pasta de plugins que contém os passes responsáveis por enriquecer os nós (\texttt{names\_visibility}, \texttt{method\_kind}, \texttt{class\_kind}, \texttt{docs\_comments}, \texttt{naming}, \texttt{io\_signature}, \texttt{path\_info}, etc.). Cada plugin conhece apenas a API de \texttt{Ctx} e \texttt{TNode}, e não interage diretamente com outros plugins.
  \item \textbf{\texttt{pass\_plugins.pass\_registry}}: módulo que registra os passes disponíveis, resolve dependências entre eles e determina a ordem de execução. Funciona como um \emph{registry} e uma pequena camada de \emph{inversão de controle}: o usuário informa quais passes carregar e o \texttt{PassRegistry} garante que sejam aplicados na ordem correta.
  \item \textbf{\texttt{src.service}}: módulo de alto nível que oferece operações como \texttt{analyze\_path} e \texttt{export\_json}. Ele atua como \emph{fachada} do ASTCore, orquestrando carregamento de plugins, travessia da AST, enriquecimento dos nós e serialização dos resultados.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{drawio/Images/AST_Model_Plugins-plugins_package.drawio.png}
  \caption{Arquitetura modular do ASTCore.}
  \label{fig:arquitetura_astcore}
\end{figure}

A arquitetura de microkernel adotada permite que novos passes sejam adicionados, removidos ou reorganizados sem alterar o restante do sistema. Cada \texttt{pass} é uma unidade autônoma que:

\begin{itemize}
\item consome apenas \texttt{TNode} e \texttt{Ctx};
\item não conhece nem depende dos outros passes;
\item é registrado dinamicamente no \texttt{PassRegistry};
\item declara o conjunto de atributos que produz;
\item define suas pré-condições via \texttt{when}.
\end{itemize}

Isso facilita:

\begin{itemize}
\item extensão incremental do sistema;
\item experimentação rápida (adição de passes sem recompilar);
\item isolamento das análises;
\item depuração sistemática.
\end{itemize}
\subsection{Geração de embeddings a partir dos \texttt{TNode}s}

A representação vetorial dos nós da AST ocorre em duas etapas. Na primeira, cada \texttt{TNode} é codificado individualmente em um vetor $d$-dimensional que combina características estruturais e textuais. Na segunda etapa, o grafo da AST recebe pesos escalares em suas arestas, calculados por funções gaussianas que dependem da distância relativa entre os nós (em profundidade e/ou número de linha). Essa etapa é inspirada no uso de \emph{Gaussian embedders} proposto por \textcite{LiangHuang2024ASTLLM}, que empregam núcleos gaussianos para codificar posições relativas em modelos \emph{Transformer}; neste trabalho, a mesma ideia é adaptada ao contexto de grafos, sendo aplicada como viés multiplicativo nas arestas durante a agregação de mensagens da GNN.

\subsubsection{Embedding por nó: estrutura e texto}

A função \texttt{build\_node\_feature\_matrix} transforma a lista de \texttt{TNode}s em uma matriz de atributos \(X \in \mathbb{R}^{N \times d}\), em que cada linha corresponde ao embedding de um nó.

O vetor de entrada de cada nó é construído a partir de dois grupos de atributos estruturais e textuais.

O vetor estrutural contém:
\begin{itemize}
\item variáveis categóricas one-hot (tipo AST, visibilidade, classe\/método, core\_kind);
\item variáveis contínuas normalizadas (profundidade, número de parâmetros, número de raises);
\item flags booleanas.
\end{itemize}

Essa combinação de atributos discretos e contínuos permite que a GNN capture:

\begin{itemize}
\item sinais sintáticos (estrutura do código),
\item sinais semânticos (uso de classes, métodos, padrões de chamada),
\item e sinais hierárquicos (profundidade, escopo).
\end{itemize}

O texto derivado de cada \texttt{TNode}, produzido por \texttt{concat\_text\_from\_tnode}, fornece um resumo semântico rico que inclui:
\begin{itemize}
\item nome qualificado do símbolo;
\item tokens do identificador (snake\/camel split);
\item assinaturas e parâmetros;
\item docstrings e comentários;
\item informação de core\_kind.
\end{itemize}

Esse texto é codificado por um encoder neural leve (\texttt{TNodeMLPEncoder}), permitindo capturar semântica local sem necessidade de modelos de linguagem pesados.

Os vetores estrutural e textual são então concatenados e passados por uma MLP (\texttt{TNodeMLPEncoder}), definida em \texttt{tnode\_encoder.py}, que projeta o resultado em um espaço de dimensão \(d\) (neste trabalho, \(d = 128\)):

\[
\text{encode\_tnode}(t)
= \text{MLP}\big(\text{structural\_features}(t) \mathbin{\|} \text{text\_features}(t)\big)
\in \mathbb{R}^d.
\]

A matriz final de atributos de nó é dada por
\[
X = \mathrm{stack}\big(\{\text{encode\_tnode}(t) \mid t \in \texttt{tnodes}\}\big)
\in \mathbb{R}^{N \times d},
\]
combinando simultaneamente propriedades sintáticas (AST) e semânticas (texto).

\subsubsection{Projeção Sintática Reduzida}
\label{subsubsec:projecao_sintatica_reduzida}
Embora a AST completa seja construída e enriquecida pelos passes, a etapa de geração de embeddings utiliza apenas uma \textbf{subárvore reduzida}, alinhada à proposta de \textcite{tonella2007reverse}, contendo apenas os nós essenciais ao fluxo de objetos e à extração de comportamentos:

\[
\mathrm{core\_tnodes} = \{\, t \in \mathrm{TNodes} \mid t.\mathrm{is\_core} = \text{True} \,\}.
\]

A redução preserva apenas:

\begin{itemize}
\item nós de escopo (\texttt{Module}, \texttt{ClassDef}, \texttt{FunctionDef});
\item atribuições (\texttt{Assign}, \texttt{AnnAssign});
\item chamadas (\texttt{Call});
\item retornos (\texttt{Return});
\item exceções (\texttt{Raise}).
\end{itemize}

Nós de controle, como \texttt{If}, \texttt{For}, \texttt{While} e \texttt{Try}, permanecem disponíveis na AST completa, mas são excluídos da projeção reduzida, de modo que apenas operações relevantes ao fluxo semântico participem da etapa de aprendizado.

Esse design permite manter o máximo de informação estrutural nos \texttt{TNode}s, ao mesmo tempo em que produz uma representação compacta e semanticamente orientada para a GNN.

\subsubsection{Grafo da AST e pesos gaussianos nas arestas}

A estrutura do arquivo-fonte é modelada como um grafo não direcionado, em que os vértices são os \texttt{TNode}s e as arestas representam relações pai–filho da AST. A função \texttt{build\_ast\_edge\_index} produz a matriz de adjacência esparsa no formato do PyTorch Geometric,
\(\texttt{edge\_index} \in \mathbb{N}^{2 \times E}\), contendo arestas bidirecionais.

Para enriquecer essas conexões, cada aresta \((i, j)\) recebe um peso escalar \(w_{ij}\) calculado a partir de uma função gaussiana sobre uma noção de distância posicional entre os nós. Inspirado em esquemas de \emph{relative position encoding} com kernels gaussianos para código-fonte, o peso é definido como
\[
w_{ij}
= \exp\!\left(
    - \frac{\lvert p_i - p_j \rvert^2}{2\sigma^2}
  \right),
\]
em que \(p_i\) e \(p_j\) são escalares que codificam posição: no protótipo, podem representar (i) a profundidade do arquivo na hierarquia de diretórios do repositório (armazenada no campo \texttt{depth} do \texttt{TNode}) ou (ii) o número de linha no arquivo (\texttt{lineno}), e \(\sigma\) é um hiperparâmetro de escala. Essa lógica é implementada nas funções \texttt{gaussian\_edge\_weights} e \texttt{gaussian\_edge\_weights\_combined}. A versão combinada utiliza duas dimensões (profundidade e linha), agregando os respectivos pesos (por exemplo, via produto ou média), de modo que a proximidade na organização do projeto (arquivos no mesmo subdiretório) e a proximidade lexical dentro do arquivo contribuam conjuntamente para a força da aresta.

Na prática, esses pesos atuam como um filtro de atenção estrutural: a GNN é incentivada a concentrar a agregação de mensagens em interações locais (por exemplo, entre instruções próximas dentro do mesmo arquivo ou entre símbolos definidos em arquivos vizinhos na mesma pasta), enquanto conexões entre nós muito distantes na escala escolhida são progressivamente atenuadas. Isso é desejável nesta tarefa porque grande parte da semântica relevante para compreender um símbolo está em sua vizinhança imediata — seja no entorno das suas declarações e usos, seja no contexto modular onde o arquivo está inserido.

Dessa forma, o grafo deixa de ser puramente não ponderado e passa a carregar um campo contínuo de relacionamentos estruturais: nós próximos (na mesma região do arquivo ou do repositório) recebem pesos mais altos, enquanto nós distantes são atenuados.

\subsubsection{Entrada para a GNN}

A AST completa já foi percorrida pelos \emph{passes} e cada nó foi encapsulado em um \texttt{TNode} enriquecido com metadados estruturais e semânticos. No entanto, a camada de GNN não opera sobre a árvore inteira, mas apenas sobre a projeção sintática reduzida definida na Seção~\ref{subsubsec:projecao_sintatica_reduzida}. Seja

\[
\mathcal{V}_{\text{core}} = \{\, t \in \texttt{TNodes} \mid t.\texttt{is\_core} = \text{True} \,\}
\]

o subconjunto de nós essenciais (\texttt{core\_tnodes}). Nesta etapa, todos os tensores $X$, \texttt{edge\_index} e \texttt{edge\_weight} são construídos \textbf{exclusivamente} sobre o subgrafo induzido por $\mathcal{V}_{\text{core}}$.

O modelo passa então a dispor de três elementos:

\begin{itemize}
  \item \(X \in \mathbb{R}^{N \times d}\): matriz de \emph{features} iniciais dos
        \(N = |\mathcal{V}_{\text{core}}|\) nós essenciais;
  \item \(\texttt{edge\_index} \in \mathbb{N}^{2 \times E}\): topologia do grafo reduzido, codificada como pares de índices inteiros referindo-se a nós em \(\mathcal{V}_{\text{core}}\);
  \item \(\texttt{edge\_weight} \in \mathbb{R}^{E}\): pesos gaussianos associados a cada
        aresta do grafo reduzido.
\end{itemize}

Esses tensores são então passados para o modelo \texttt{CodeGNN}:

\[
\texttt{node\_embs},\ \texttt{graph\_emb}
= \mathrm{CodeGNN}(X, \texttt{edge\_index}, \texttt{edge\_weight}),
\]

em que \texttt{node\_embs} representa as embeddings contextuais refinadas de cada nó após a propagação de mensagens, e \texttt{graph\_emb} é obtido por um \emph{pooling} global com atenção. Em vez de utilizar apenas a média simples, este trabalho emprega um mecanismo de \textbf{attention pooling}, no qual a contribuição de cada nó para o embedding global é ponderada por um peso de atenção aprendido sobre os nós em \(\mathcal{V}_{\text{core}}\). Na prática, isso faz com que o embedding de arquivo privilegie nós que concentram interações relevantes para a tarefa de redocumentação, facilitando, por exemplo, identificar trechos candidatos a representar operações centrais em casos de uso ou outros artefatos de alto nível derivados do código.

\subsubsection{Grafo de chamadas e métricas de fan-in/fan-out}

Além das informações estruturais da AST, a ferramenta constrói um grafo estático de chamadas entre funções e métodos do programa. Nesse grafo, cada nó corresponde a um elemento chamável (função ou método) e cada aresta dirigida \( (c \rightarrow d) \) indica que o chamador \(c\) realiza uma chamada para o alvo \(d\) em seu corpo.

A extração desse grafo é realizada em duas etapas, implementadas como \emph{passes} sobre os \texttt{TNode}s:

\begin{enumerate}
  \item \textbf{Coleta local de chamadas (\texttt{call\_edges})}: 
  para cada nó que representa uma definição de função ou método (isto é, nós \texttt{ast.FunctionDef} e \texttt{ast.AsyncFunctionDef}), o sistema percorre o corpo da definição em busca de nós \texttt{ast.Call}. De cada chamada é extraída uma representação textual do alvo (por exemplo, \texttt{"helper"}, \texttt{"self.level\_up"}, \texttt{"grupo.accept"}), que é armazenada em um campo \texttt{local\_callees} do \texttt{TNode}. Ao mesmo tempo, são registrados pares \((\text{caller\_id}, \text{callee\_repr})\) em uma estrutura temporária no contexto de análise (\texttt{Ctx}).

  \item \textbf{Consolidação global de métricas (\texttt{call\_metrics})}: 
  após a visita de todo o módulo (fase \texttt{POST}), esses pares são agregados para construir uma visão global do grafo de chamadas. Primeiro, cada \texttt{TNode} chamável recebe um identificador estável \(\text{caller\_id}\) (priorizando \texttt{qname} quando disponível, como \texttt{Unit.accept}). Em seguida, o sistema:
  \begin{itemize}
    \item acumula, para cada chamador, o conjunto de alvos distintos que ele chama (\texttt{callees\_set});
    \item tenta casar cada alvo textual com um chamável conhecido, usando o \emph{nome simples} como chave (por exemplo, tanto \texttt{"self.level\_up"} quanto \texttt{"Unit.level\_up"} são associados ao método \texttt{level\_up}).
  \end{itemize}
\end{enumerate}

A partir dessa agregação, são preenchidos os campos:

\begin{itemize}
  \item \texttt{callees}: lista ordenada de identificadores dos nós que \emph{este} nó chama;
  \item \texttt{callers}: lista ordenada de identificadores dos nós que \emph{chamam este} nó;
  \item \texttt{fan\_out}: número de alvos distintos chamados por esse nó (tamanho de \texttt{callees});
  \item \texttt{fan\_in}: número de chamadores distintos que atingem esse nó (tamanho de \texttt{callers}).
\end{itemize}

Formalmente, se denotarmos por \(C(m)\) o conjunto de chamadores de um método \(m\) e por \(D(m)\) o conjunto de métodos chamados por \(m\), temos:
\[
\text{fan\_in}(m) = \lvert C(m) \rvert, 
\qquad
\text{fan\_out}(m) = \lvert D(m) \rvert.
\]

Essas métricas fornecem um sinal estrutural importante para a identificação de candidatos a casos de uso:

\begin{itemize}
  \item métodos com \emph{fan-out} elevado tendem a atuar como \emph{orquestradores}, coordenando múltiplas operações internas;
  \item métodos com \emph{fan-in} elevado tendem a ser \emph{serviços reutilizados}, chamados por diversos pontos do sistema, o que os torna bons candidatos a operações incluídas (\texttt{include}) em vários casos de uso;
  \item métodos com \(\texttt{fan\_in} = \texttt{fan\_out} = 0\) tendem a ser folhas isoladas, com menor relevância para a visão de alto nível do comportamento do sistema.
\end{itemize}

Na etapa de recuperação de casos de uso, esses valores de \texttt{fan\_in} e \texttt{fan\_out} são combinados com outros atributos (como visibilidade, tipo de classe e embeddings gerados pela GNN) para priorizar métodos mais centrais como candidatos a casos de uso principais e para destacar operações com alto grau de reutilização como candidatas a relacionamentos de inclusão.

\subsubsection{Cálculo de métricas estruturais dos casos de uso}

A partir do grafo de dependências entre os métodos públicos selecionados como candidatos a Casos de Uso (UCs) (Seção~\ref{sec:callgraph}), a ferramenta calcula um conjunto de métricas estruturais derivadas exclusivamente da análise estática do código-fonte. Essas métricas permitem identificar padrões de reutilização, dependência e papel arquitetural de cada UC dentro do sistema. O cálculo é realizado pela função \texttt{compute\_uc\_metrics}, que recebe como entrada o conjunto de arestas dirigidas entre UCs:
\[
(\mathrm{UC}_{\text{src}}, \mathrm{UC}_{\text{dst}}) \in E,
\]
onde \(\mathrm{UC}_{\text{src}}\) chama \(\mathrm{UC}_{\text{dst}}\) em seu corpo.

\paragraph{Fan-in e fan-out}

Para cada UC \(u\), são computados:

\begin{itemize}
  \item \textbf{Fan-in}: número de UCs que chamam \(u\):
  \[
    \mathrm{fan\_in}(u) = \bigl\lvert \{\, v \mid (v \rightarrow u) \in E \,\} \bigr\rvert;
  \]
  \item \textbf{Fan-out}: número de UCs que \(u\) chama:
  \[
    \mathrm{fan\_out}(u) = \bigl\lvert \{\, w \mid (u \rightarrow w) \in E \,\} \bigr\rvert.
  \]
\end{itemize}

Essas métricas são clássicas na análise de arquitetura e refletem o grau de acoplamento entre responsabilidades.

\paragraph{Normalização dos valores}

Como diferentes sistemas podem ter escalas muito distintas (por exemplo, módulos com poucos UCs e módulos com centenas), os valores de \(\mathrm{fan\_in}\) e \(\mathrm{fan\_out}\) são normalizados para o intervalo \([0, 1]\):
\[
  \mathrm{in\_norm}(u) = 
  \frac{\mathrm{fan\_in}(u)}{\max_{x} \mathrm{fan\_in}(x)}, 
  \qquad
  \mathrm{out\_norm}(u) = 
  \frac{\mathrm{fan\_out}(u)}{\max_{x} \mathrm{fan\_out}(x)}.
\]
Essa normalização torna a análise menos sensível ao tamanho absoluto do projeto, permitindo comparar UCs em sistemas de porte diferente.

\paragraph{Score base de inclusão}

Em UML, um caso de uso tende a ser incluído por outro quando representa uma funcionalidade reutilizável, relativamente autocontenida e invocada repetidamente por diferentes UCs externos. Inspirando-se nesse comportamento, a ferramenta define um \emph{score base de inclusão} para cada UC \(u\):
\[
  \mathrm{include\_base}(u)
  = \mathrm{in\_norm}(u) \cdot 
    \bigl(1 - \lambda_{\text{out}} \cdot \mathrm{out\_norm}(u)\bigr),
\]
em que \(\lambda_{\text{out}} \in [0, 1]\) controla quanto o \emph{fan-out} influencia negativamente o score (neste trabalho, adotou-se \(\lambda_{\text{out}} = 0{,}7\)). Nessa formulação:
\begin{itemize}
  \item \(\mathrm{in\_norm}(u)\) aumenta a pontuação de UCs amplamente reutilizados;
  \item \(\mathrm{out\_norm}(u)\) penaliza UCs que orquestram muitas dependências, comportamento mais típico de casos de uso ``macro'', e não de UCs incluídos.
\end{itemize}

\paragraph{Bônus estrutural para UCs auxiliares}

Além do score base, o algoritmo acrescenta um \emph{bônus estrutural} associado ao papel de UC auxiliar (\emph{helper}). Esse bônus cresce quando o UC é alvo de diversas chamadas, mas possui \emph{fan-out} baixo:
\[
  \mathrm{bonus}(u) 
  = \beta \cdot 
    \frac{\mathrm{fan\_in}(u)}
         {\mathrm{fan\_in}(u) + \mathrm{fan\_out}(u) + \gamma},
\]
em que \(\beta\) é um fator de escala (neste trabalho, \(\beta = 0{,}1\)) e \(\gamma > 0\) evita divisão por zero e estabiliza a fórmula. Esse padrão caracteriza UCs folha reutilizados, fortemente associados ao estereótipo \texttt{\textless\textless include\textgreater\textgreater}.

\paragraph{Score final e relação com estereótipos UML}

O score final de inclusão é obtido pela combinação do score base com o bônus estrutural:
\[
  \mathrm{include\_score}(u) 
  = \min\bigl(\mathrm{include\_base}(u) + \mathrm{bonus}(u),\ 1{,}0\bigr),
\]
garantindo um valor sempre no intervalo \([0, 1]\).

Na etapa de recuperação de casos de uso, scores altos de \(\mathrm{include\_score}(u)\) indicam UCs que se comportam como serviços reutilizados por diversos outros UCs e, portanto, bons candidatos a relacionamentos \texttt{\textless\textless include\textgreater\textgreater}. Scores muito baixos tendem a caracterizar UCs pouco reutilizados ou UCs raiz (\emph{entry}), enquanto scores intermediários, combinados com outras evidências (como condições de disparo observadas no código), podem sugerir candidatos a \texttt{\textless\textless extend\textgreater\textgreater}. Esses valores são combinados posteriormente com outros atributos (visibilidade, tipo de classe e embeddings gerados pela GNN) para orientar a clusterização e priorização de casos de uso.


\section{Escolha dos repositórios}

Para a análise foram escolhidos três repositórios independentes, dois de David Beazley e um de Brandon Rhodes, duas referências em linguagem Python. Os repositórios de David Beazley possuem documentação completa no próprio repositório, facilitando a compreensão do software construído. Já o repositório de Brandon Rhodes não contém documentação, mas seu conteúdo é a adaptação do jogo \emph{Colossal Cave Adventure} de Fortran para Python.


\subsection{Colossal Cave Adventure}
Este trabalho utiliza como base uma reimplementação de \textcite{rhodes_adventure_py} em Python 3, que preserva o jogo original de Crowther e Don Woods, utilizando o arquivo de dados \texttt{advent.dat} \textcite{adventure_original_sources}. O pacote permite jogar em dois modos, no \emph{prompt} do Python e em terminal do sistema operacional. Além disso, disponibiliza \textit{walkthroughs} automatizados na pasta de testes.

\subsection{Descrição do jogo}

\textit{Colossal Cave Adventure}, também conhecido como \textit{ADVENT} ou simplesmente \textit{Adventure}, é amplamente reconhecido como o primeiro jogo de aventura baseado em texto da história, criado por Will Crowther em meados de 1975 e expandido por Don Woods em 1976. 

Ambientado em uma caverna repleta de tesouros, criaturas e labiríntos, o jogador interage por comandos de texto, como \textit{"GO NORTH"} ou \textit{"GET LAMP"}. O sistema responde com descrições que narram as consequências das ações.

Como observa \textcite{dibbell1998mytinylife}, o jogo automatiza o papel do mestre (\textit{Dungeon Master}) característico de campanhas de \textit{Dungeons and Dragons}. Suas descrições textuais simulam a fala do mestre (“\textit{YOU ARE IN A MAZE OF TWISTY LITTLE PASSAGES, ALL ALIKE}”).  

“Como qualquer programa significativo, \textit{Adventure} expressava a personalidade e o ambiente de seus autores.” \textcite{levy2010hackers}

Will Crowther e sua ex-esposa, Patricia Crowther, ambos programadores e espeleólogos, participaram do mapeamento do sistema de cavernas \textit{Mammoth Cave}. No verão de 1974, enquanto jogava campanhas de \textit{Dungeons and Dragons}, Will começou o desenvolvimento do seu jogo utilizando o Fortran. O mapa utilizado no jogo foi inspirado diretamente nos levantamentos realizados pelo casal durante as expedições à Mammoth Cave, construindo no código a estrutura real da caverna.

Como o próprio Will Crowther relata, a ideia do jogo surgiu da combinação entre suas experiências em espeleologia e seu interesse por \textit{Dungeons and Dragons}: “Eu estava envolvido em um jogo de interpretação de papéis... e tive uma ideia que combinasse o meu interesse por exploração de cavernas com algo que também fosse um jogo para as crianças...” \textcite{peterson1983genesis}.

\textcite{levy2010hackers} conta como inicia a colaboração de Donald Woods, um pesquisador da \textit{Stanford Artificial Intelligence Laboratory} (SAIL), em 1976. Após ter contato com uma prévia do jogo, Woods entrou em contato com Crowther, obteve sua permissão e passou a expandir o código. Sua versão incorporou novos puzzles, criaturas e elementos de fantasia inspirados na obra de Tolkien, além de um sistema de pontuação que estabelecia um objetivo ao jogador. A versão combinada de Crowther e Woods é um marco na história da interação humano-computador.

\subsection{}
Como o jogo não possui documentação original, utilizei o artigo de \textcite{jerz2007colossal} como referência para compreender a estrutura e o funcionamento do código. O autor recupera e examina o código-fonte escrito por Will Crowther, a partir de um backup preservado no SAIL. Jerz descreve as seis tabelas centrais que organizam os dados do jogo: descrições longas, rótulos curtos das salas, dados de mapa, vocabulário agrupado, estados estáticos e eventos ou dicas.  

Essa arquitetura de dados é mantida na reimplementação em Python, embora expandida para doze seções, resultado da integração da versão de Don Woods \textcite{rhodes_adventure_py}. A leitura e o processamento dessas tabelas ocorrem por meio do arquivo \texttt{advent.dat}, que preserva a semântica e a estrutura do código original. 

As seis tabelas descritas por Crowther estruturam o mundo do jogo e suas interações:
\begin{enumerate}
    \item \textbf{Long Descriptions}: textos descritivos longos que definem os ambientes e estados narrativos;
    \item \textbf{Short Room Labels}: nomes curtos usados internamente para identificar locais e facilitar a navegação;
    \item \textbf{Map Data}: conexões topológicas entre os ambientes e as direções de movimento possíveis;
    \item \textbf{Grouped Vocabulary Keywords}: agrupamento de palavras-chave e comandos interpretados pelo sistema;
    \item \textbf{Static Game States}: variáveis e condições fixas que controlam a lógica do jogo;
    \item \textbf{Hints and Events}: mensagens de ajuda, eventos dinâmicos e respostas a situações específicas.
\end{enumerate}

As outras seis adicionadas na versão em colaboração com Woods são:

\begin{enumerate}
  \item \emph{Object locations} — localização dos objetos;
  \item \emph{Action defaults} — mensagens padrão ligadas a verbos de ação;
  \item \emph{Liquid assets / flags} — \texttt{COND} por sala (luz, líquidos, restrições do pirata, bits de dicas);
  \item \emph{Class messages} — faixas de pontuação e mensagens de classificação do jogador;
  \item \emph{Hints} — dicas (turnos necessários, penalidade, pergunta e resposta);
  \item \emph{Magic messages} — mensagens de inicialização e manutenção.
\end{enumerate}

\paragraph{Tabela 1 – Long Descriptions.}  
A Tabela 1 contém descrições extensas dos ambientes do jogo. Com entradas identificadas de 1 a 140, ela define os textos apresentados ao jogador em diferentes locais. Cada linha representa uma sala ou estado narrativo. Parte dessas descrições refere-se diretamente a locais da caverna, como o trecho “\textit{YOU ARE STANDING AT THE END OF A ROAD BEFORE A SMALL BRICK BUILDING}”, enquanto outras descrevem situações de falha ou eventos inesperados, como “\textit{YOU ARE AT THE BOTTOM OF THE PIT WITH A BROKEN NECK}”.  

Exemplos:  
\begin{itemize}
    \item 1	\textit{AROUND YOU IS A FOREST.  A SMALL STREAM FLOWS OUT OF THE BUILDING AND DOWN A GULLY}.
    \item 2	\textit{YOU HAVE WALKED UP A HILL, STILL IN THE FOREST.  THE ROAD SLOPES BACK DOWN THE OTHER SIDE OF THE HILL.  THERE IS A BUILDING IN THE DISTANCE.}
    \item 3	\textit{YOU ARE INSIDE A BUILDING, A WELL HOUSE FOR A LARGE SPRING.}
\end{itemize}

\paragraph{Tabela 2 – Short Room Labels.}  
A Tabela 2 contém rótulos curtos correspondentes às localizações/ambientes do jogo. Com entradas numeradas de 1 a 130, nem todas as salas ou estados definidos em \textit{Long Descriptions} possuem equivalentes resumidos.  

Exemplos:  
\begin{itemize}
    \item 1 \textit{YOU'RE AT END OF ROAD AGAIN.}
    \item 3 \textit{YOU'RE INSIDE BUILDING.}
    \item 18 \textit{YOU'RE IN NUGGET OF GOLD ROOM.}
    \item 19 \textit{YOU'RE IN HALL OF MT KING.}
\end{itemize}

\paragraph{Tabela 3 – Map Data.}
A Tabela 3 codifica a topologia do mundo do jogo e as regras de navegação, funcionando como um grafo dirigido rotulado. A primeira coluna indica o ambiente em que o jogador se encontra, a segunda define o ambiente de destino, e as colunas subsequentes agrupam os vocabulários que podem ser utilizados para realizar a transição entre os dois pontos. O mapeamento dos vocabulários é definido na Tabela 4.  

Em alguns casos, o valor do destino representa uma condição especial, e não uma simples sala. Se o número de destino for maior que 500, o jogo exibe uma mensagem da Tabela 6 e o jogador permanece no mesmo local; Se estiver entre 300 e 500, o valor indica um salto especial para um trecho de código do jogo.
   
Exemplos:
\begin{itemize}
  \item 1  2  2  44  29: o jogador se desloca do ambiente 1 ao aombiente 2, se utilizados os comando 2, 44 ou 29. 
  \item 3  1  3  11  32  44: o jogador se desloca do ambiente 2 ao ambiente 1 se utilizados os comando 3, 11, 32 ou 44.
\end{itemize}

\paragraph{Tabela 4 – Grouped Vocabulary Keywords.}
No código original em Fortran, toda entrada de texto era truncada nos cinco primeiros caracteres, de modo que o comando \textit{“inventory”}, por exemplo, poderia ser digitado simplesmente como \textit{“inven”}. A reimplementação em Python de \textcite{rhodes_adventure_py} preserva essa lógica.  

Os dados da tabela 4 são divididos em 4 grupos: o primeiro com id's entre 1 e 100 para movimento no jogo; com ids entre 1000 e 2000, trata de objetos manipuláveis ou características de cenário; com ids entra 2000 e 3000 são verbos de ação, se entre 3000 e 4000 são para casos especiais.

\begin{itemize}
  \item 1–100: verbos de movimento, utilizados para navegação no espaço do jogo;  
  \item 1000–2000: objetos e elementos de cenário manipuláveis;  
  \item 2000–3000: verbos de ação (\textit{carry}, \textit{attack}, \textit{drop}, etc.);  
  \item 3000–4000: verbos de casos especiais, geralmente associados a eventos ou mensagens específicas definidas na Tabela 6.  
\end{itemize}

Além dos comandos clássicos de navegação por bússola, "\textit{EAST}"/"\textit{E}", "\textit{WEST}"/"\textit{W}", "\textit{NORTH}"/"\textit{N}", "\textit{SOUTH}"/"\textit{S}", parte dos veros de movimentos são nomes de locais da caverna como "\textit{BEDQU}" (truncamento de \textit{Bedquilt}), "\textit{HOUSE}", "\textit{GATE}" e "\textit{FORES}" (\textit{forest}).

Exemplos:  
\begin{itemize}
    \item 2 \textit{ROAD}
    \item 3 \textit{ENTER}
    \item 3 \textit{DOOR}
    \item 3 \textit{GATE}
    \item 4 \textit{UPSTR}
    \item 5 \textit{DOWNS}
    \item 6 \textit{FORES}
\end{itemize}

Palavras de mesmo sentido/sinônimos possuem mesmo id, como "\textit{ENTER}", "\textit{DOOR}" e "\textit{GATE}".

\paragraph{Tabela 5 – Static Game States.}  
A Tabela 5 armazena descrições curtas que representam estados do jogo, correspondendo às mudanças permanentes no ambiente. Cada linha contém um número e uma mensagem descritiva.  

Quando o identificador está entre 1 e 100, a linha define a mensagem de inventário associada a um objeto, exemplo: “\textit{SET OF KEYS}” se refere a "\textit{KEYS}". Quando o identificador é um múltiplo de 100, a mensagem descreve uma propriedado do objeto. 

Exemplos:  
\begin{itemize}
    \item 1	SET OF KEYS
    \item 000	THERE ARE SOME KEYS ON THE GROUND HERE.
    \item 2	BRASS LANTERN
    \item 000	THERE IS A SHINY BRASS LAMP NEARBY.
    \item 100	THERE IS A LAMP SHINING NEARBY.
    \item 3	*GRATE
    \item 000	THE GRATE IS LOCKED.
    \item 100	THE GRATE IS OPEN.
\end{itemize}

\paragraph{Tabela 6 – Hints and Events.}
A Tabela 6 reúne mensagens arbitrárias usadas como dicas e como descrições de eventos pontuais. Essas mensagens não estão relacionadas a um ambiente ou objeto específicos, elas são acionadas por outras estruturas do jogo, como as tabelas 3, 4, 8 e 11.

Exemplos:
\begin{enumerate}
    \item 3	AXE AT YOU WHICH MISSED, CURSED, AND RAN AWAY.
    \item 6	NONE OF THEM HIT YOU!
    \item 13 I DON'T UNDERSTAND THAT!
    \item 24 YOU ARE ALREADY CARRYING IT!
    \item 33 I DON'T KNOW HOW TO LOCK OR UNLOCK SUCH A THING.
\end{enumerate}

\paragraph{Tabela 7 – Object Locations.}
A Tabela 7 define onde cada objeto surge no mundo do jogo e se ele é móvel ou fixo. Cada linha possui o identificador do objeto, a sala inicial, e um campo opcional que indica imobilidade (-1) ou uma segunda sala quando o objeto existe simultaneamente em dois lugares 

\begin{itemize}
  \item Sala inicial = 0: o objeto não aparece no mundo no início e só será criado por algum evento ou ação do jogador.
  \item Terceiro campo = -1: o objeto está fixo naquela sala (não pode ser carregado).
  \item Terceiro campo = número de sala: o objeto está presente em duas salas ao mesmo tempo, objetos com duas localizações são tratados como imóveis.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 1 3: objeto 1 (1001 - KEY, KEYS) começam na sala 3 (INSIDE BUILDING).
  \item 2 3: objeto 2 (1002 - LAMP, HEADL, LANTE) começam na sala 3 (INSIDE BUILDING).
  \item 3 8 9: objeto 3 (1003 - grate) existe nas salas 8 e 9 simultaneamente (8 - YOU'RE OUTSIDE GRATE, 9 - YOU'RE BELOW THE GRATE.).
  \item (9 - DOOR) (94 - YOU ARE AT ONE END OF AN IMMENSE NORTH/SOUTH PASSAGE.)
  \item 9 94 -1: objeto 9 (1009 - DOOR) é fixo na sala 94 (94 - YOU ARE AT ONE END OF AN IMMENSE NORTH/SOUTH PASSAGE.).
  \item 15 0: objeto 15 (1015 - OYSTE) começa fora do mundo e aparece mais tarde.
\end{itemize}

\paragraph{Tabela 8 – Action Defaults.}  
A Tabela 8 define o comportamento padrão dos verbos de ação, associando cada identificador de verbo ao índice da mensagem correspondente na Tabela 6. Cada linha contém dois valores: o primeiro é o número do verbo de ação, e o segundo é o identificador da mensagem padrão que deve ser exibida.

Exemplos:  
\begin{itemize}
  \item 1 24: o verbo de ação associado ao id 1 (2001 - CARRY, TAKE, KEEP, CATCH, STEAL, CAPTU, GET, TOTE) e a mensagem 24 da tabela 6 (YOU ARE ALREADY CARRYING IT!). 
  \item 6 33: o verbo de ação associado ao id 6 (2006 - LOCK, CLOSE) e a mensagem 33 da tabela 6 (I DON'T KNOW HOW TO LOCK OR UNLOCK SUCH A THING.).
  \item 7 38: o verbo de ação associado ao id 7 (2007 - LIGHT, ON) e a mensagem 38 da tabela 6 (YOU HAVE NO SOURCE OF LIGHT.). 
\end{itemize}

\paragraph{Tabela 9 – Liquid Assets, Etc.}
A Tabela 9 define os bits de condição associados a cada sala, controlando luz, líquidos, presença de inimigos e zonas de interesse para as rotinas de dicas. Cada linha contém um identificador de bit e uma lista de até vinte localizações nas quais esse bit é ativado. O jogo usa esses bits para determinar o comportamento dinâmico de cada ambiente.

\begin{itemize}
  \item 0: indica que o ambiente está naturalmente iluminado.
  \item 1: tipo de líquido usado em conjunto com o bit 2. Quando o bit 2 está ativo, este bit diferencia óleo (1) de água (0).
  \item 2: marca as salas que contêm água ou óleo.
  \item 3: impede que o pirata apareça ali, exceto quando persegue o jogador.
  \item 4: jogador tentando entrar na caverna.
  \item 5: tentativa de capturar o pássaro.
  \item 6: interação com a cobra.
  \item 7: perdido no labirinto.
  \item 8: refletindo no quarto escuro.
  \item 9: na área final Witt's End.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 0 1 2 3 4 5 6 7 8 9 10 100 115 116 126: salas naturalmente iluminadas próximas à entrada.
  \item 2 1 3 4 7 38 95 113 24: presença de líquido (água ou óleo) nessas salas.
  \item 9 108: marca a área final do jogo, Witt’s End.
\end{itemize}

\paragraph{Tabela 10 – Class Messages.}
A Tabela 10 contém as mensagens de classificação do jogador de acordo com a pontuação total atingida ao final da partida. Cada linha associa um limite superior de pontuação a uma mensagem que descreve o título ou o nível de habilidade alcançado.  

Exemplos:
\begin{itemize}
  \item 35: YOU ARE OBVIOUSLY A RANK AMATEUR.  BETTER LUCK NEXT TIME.
  \item 100: YOUR SCORE QUALIFIES YOU AS A NOVICE CLASS ADVENTURER.
  \item 130: YOU HAVE ACHIEVED THE RATING: ‘EXPERIENCED ADVENTURER’.
  \item 200: YOU MAY NOW CONSIDER YOURSELF A ‘SEASONED ADVENTURER’.
  \item 250: YOU HAVE REACHED ‘JUNIOR MASTER’ STATUS.
  \item 300: MASTER ADVENTURER CLASSES C.
  \item 330: MASTER ADVENTURER CLASSES B.
  \item 349: MASTER ADVENTURER CLASSES A.
  \item 9999: ALL OF ADVENTUREDOM GIVES TRIBUTE TO YOU, ADVENTURER GRANDMASTER!
\end{itemize}

\paragraph{Tabela 11 – Hints.}
A Tabela 11 associa dicas contextuais a condições determinadas de jogo.  
Cada linha contém cinco valores:  

\begin{itemize}
  \item O primeiro valor vincula a dica a uma condição definidos na Tabela 9.
  \item O segundo valor define quantos turnos o jogador deve gastar no mesmo estado antes da dica ser oferecida.
  \item O terceiro valor representa a penalidade subtraída da pontuação total ao aceitar a ajuda.
  \item Os dois últimos valores apontam para mensagens da Tabela 6: a pergunta inicial e a resposta.
\end{itemize}

Exemplos:
\begin{itemize}
  \item 4 4 2 62 63 — Bit 4 (entrada da caverna): após 4 turnos no local, o jogo exibe a pergunta 62 (Do you need help getting inside?) e, se aceita, mostra a resposta 63 (Perhaps you should explore the grate.), descontando 2 pontos.
  \item 6 8 2 20 21 — Bit 6 (cobra): depois de 8 turnos, o jogador recebe uma dica para resolver o enigma da serpente.
  \item 7 75 4 176 177 — Bit 7 (labirinto): após 75 turnos perdido, é oferecida uma dica de saída, com penalidade de 4 pontos.
  \item 8 25 5 178 179 — Bit 8 (quarto escuro): a dica surge depois de 25 turnos, custando 5 pontos.
\end{itemize}

\paragraph{Tabela 12 – Magic Messages.}
A Tabela 12 contém as chamadas \textit{Magic Messages}, um conjunto de mensagens reservadas utilizadas pelos modos de inicialização, manutenção e administração do jogo.  
Embora seu formato seja idêntico ao da Tabela 6, elas são separadas para facilitar o acesso e o controle das rotinas especiais do sistema. Cada linha contém um identificador e um texto associado.agens internas do sistema.

Exemplos
\begin{itemize}
  \item 1 \textit{A LARGE CLOUD OF GREEN SMOKE APPEARS IN FRONT OF YOU… HE MAKES A SINGLE PASS OVER YOU WITH HIS HANDS, AND EVERYTHING FADES AWAY INTO A GREY NOTHINGNESS.}
  \item 2 \textit{EVEN WIZARDS HAVE TO WAIT LONGER THAN THAT!}
  \item 3 \textit{I'M TERRIBLY SORRY, BUT COLOSSAL CAVE IS CLOSED. OUR HOURS ARE:}
  \item 4 \textit{ONLY WIZARDS ARE PERMITTED WITHIN THE CAVE RIGHT NOW.}
\end{itemize}

\newpage

\printbibliography

\end{document}
